{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-27T21:24:04.504176Z",
     "start_time": "2024-11-27T21:23:17.509778Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"]=\"1\"\n",
    "import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.metrics.cluster import entropy\n",
    "from tf_keras import layers,losses,optimizers,Sequential\n",
    "from tf_keras.models import Model\n",
    "from tf_keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D,Input\n",
    "from tf_keras.applications import VGG16\n",
    "from tf_keras import datasets,models\n",
    "import importlib"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\neyen\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:24:04.515921Z",
     "start_time": "2024-11-27T21:24:04.511405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "id": "ac5bc649c1bea00",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:24:06.547166Z",
     "start_time": "2024-11-27T21:24:04.726133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from importing_dataset import load_dataset\n",
    "import Entropy\n",
    "importlib.reload(Entropy)\n",
    "train_examples, validation_examples, num_examples, num_classes, class_names = load_dataset('horses_or_humans', 70)"
   ],
   "id": "3dfca37d679c6ef0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from C:\\Users\\neyen\\tensorflow_datasets\\horses_or_humans\\3.0.0\n",
      "INFO:absl:Creating a tf.data.Dataset reading 2 files located in folders: C:\\Users\\neyen\\tensorflow_datasets\\horses_or_humans\\3.0.0.\n",
      "INFO:absl:Creating a tf.data.Dataset reading 1 files located in folders: C:\\Users\\neyen\\tensorflow_datasets\\horses_or_humans\\3.0.0.\n",
      "INFO:absl:Constructing tf.data.Dataset horses_or_humans for split ('train[:70%]', 'train[70%:]'), from C:\\Users\\neyen\\tensorflow_datasets\\horses_or_humans\\3.0.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:24:06.635512Z",
     "start_time": "2024-11-27T21:24:06.571775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from reformatting import reformat_image\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "train_batches = train_examples.cache().shuffle(num_examples//4).map(reformat_image).batch(BATCH_SIZE).prefetch(1)\n",
    "validation_batches = validation_examples.map(reformat_image).batch(BATCH_SIZE).prefetch(1)\n",
    "train_batches"
   ],
   "id": "cdc250b3c14c9cb1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:26:37.060261Z",
     "start_time": "2024-11-27T21:26:35.687104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from tf_keras import backend as K\n",
    "# Assurez-vous que l'entrée est un KerasTensor\n",
    "input_layer = K.placeholder(shape=(None, 224, 224, 3), dtype='float32')\n",
    "\n",
    "\n",
    "# Bloc 1\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(input_layer)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "# Bloc 2\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "# Bloc 3\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "# Bloc 4\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "# Bloc 5\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "# Ajouter des couches supplémentaires\n",
    "x = layers.GlobalMaxPooling2D(name=\"global_max_pool\")(x)\n",
    "x = layers.Dropout(0.3, name=\"dropout\")(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "# Construire le modèle\n",
    "model = Model(input_layer, outputs, name=\"custom_vgg16.keras\")\n",
    "model.summary()\n",
    "\n",
    "# Charger les poids de VGG16\n",
    "vgg16_base = VGG16(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "\n",
    "from test_surgeon import Surgeon\n",
    "# Appliquer les poids pour chaque couche manuellement\n",
    "for layer, pretrained_layer in zip(model.layers, vgg16_base.layers):\n",
    "    if isinstance(layer, layers.Conv2D):\n",
    "        layer.set_weights(pretrained_layer.get_weights())\n",
    "\n",
    "input_layer = model.input  # L'entrée du modèle\n",
    "layer_0 = model.layers[1]  # Index 1 car la première couche est 'input_40'\n",
    "surgeon = Surgeon(model)\n",
    "\n",
    "# Si la couche 0 est une Conv2D, supprimer les canaux\n",
    "if isinstance(layer_0, layers.Conv2D):\n",
    "    # Surgeon pour supprimer les canaux\n",
    "    surgeon.add_job('delete_channels', layer_0, channels=[0, 1, 2])\n",
    "    pruned_model = surgeon.operate()\n",
    "\n",
    "    # Redéfinir les entrées et sorties du modèle après modification\n",
    "    pruned_model = Model(inputs=input_layer, outputs=pruned_model.output)\n",
    "\n",
    "    # Redéfinir les entrées du modèle (pour qu'elles soient un KerasTensor valide)\n",
    "    pruned_model.build(input_shape=(None, 224, 224, 3))\n",
    "\n",
    "    # Recompiler le modèle pour qu'il soit prêt à l'entraînement ou à la prédiction\n",
    "    pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"La couche spécifiée n'est pas une couche Conv2D.\")\n",
    "\n",
    "pruned_model.summary()\n"
   ],
   "id": "861e115df888413e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_vgg16.keras\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_max_pool (GlobalMax  (None, 512)               0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14715714 (56.14 MB)\n",
      "Trainable params: 14715714 (56.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Deleting 3/64 channels from layer: block1_conv1\n",
      "WARNING:tensorflow:From C:\\Users\\neyen\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\neyen\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 61)      1708      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      35200     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_max_pool (GlobalMax  (None, 512)               0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14713902 (56.13 MB)\n",
      "Trainable params: 14713902 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:04:51.971830Z",
     "start_time": "2024-11-27T22:04:49.297599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import Entropy_Keras_Surgeon\n",
    "importlib.reload(Entropy_Keras_Surgeon)\n",
    "from Entropy_Keras_Surgeon import EntropyPruningSurgeon\n",
    "testt_model = models.load_model(\"model_experiments/custom_vgg16.keras\")\n",
    "pruning = EntropyPruningSurgeon(model=testt_model, threshold=0.026)\n",
    "pruningmodel = pruning.run()\n",
    "pruningmodel.summary()\n"
   ],
   "id": "6db2bde71aab52b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17269282 0.08593657 0.06639509 0.15977614 0.12494584 0.11560801\n",
      " 0.08723375 0.1031163  0.1304353  0.08468138 0.1758175  0.09220949\n",
      " 0.14930637 0.08204485 0.13381846 0.10183211 0.08377397 0.08114591\n",
      " 0.10910635 0.09520047 0.11484847 0.13169202 0.08065336 0.13898827\n",
      " 0.04546823 0.09654906 0.15572321 0.08630413 0.13806152 0.08832835\n",
      " 0.10611238 0.08109367 0.05137431 0.08372652 0.15003803 0.14347923\n",
      " 0.07392103 0.15742144 0.0875589  0.07336573 0.06497415 0.15371111\n",
      " 0.09803167 0.13182561 0.0832122  0.19951327 0.09752656 0.1636866\n",
      " 0.11984214 0.13067804 0.07287492 0.18149455 0.06964114 0.11167739\n",
      " 0.12075619 0.07491273 0.08534266 0.1264338  0.15043746 0.11684294\n",
      " 0.13868542 0.07888904 0.17409943 0.13589956]\n",
      "Pruned indices for layer 'block1_conv1': []\n",
      "[0.19925979 0.13174167 0.13938427 0.14332995 0.14302185 0.19506392\n",
      " 0.1656115  0.10986789 0.14186813 0.09592294 0.19612369 0.12438349\n",
      " 0.18760093 0.2106316  0.07597923 0.13958845 0.14041013 0.18103084\n",
      " 0.20223175 0.216287   0.19713873 0.13048053 0.09135111 0.08531211\n",
      " 0.15838152 0.08830558 0.19336529 0.18220645 0.20690796 0.20956509\n",
      " 0.19651073 0.12355763 0.17178541 0.18957838 0.17563823 0.19233796\n",
      " 0.11635711 0.18964432 0.10922125 0.08315296 0.17367685 0.14922924\n",
      " 0.09776839 0.13264942 0.14287299 0.18884523 0.14303216 0.20347303\n",
      " 0.11423463 0.1453212  0.17955421 0.11550456 0.19017765 0.18052828\n",
      " 0.12690438 0.19521236 0.17205867 0.18200935 0.15330634 0.22168134\n",
      " 0.18197131 0.11842461 0.18681712 0.12962595]\n",
      "Pruned indices for layer 'block1_conv2': []\n",
      "[0.04235179 0.06263747 0.05738691 0.1256169  0.09527488 0.09978005\n",
      " 0.07208151 0.10235035 0.09302627 0.07422479 0.07494155 0.09056938\n",
      " 0.12823428 0.06656099 0.10386094 0.09460416 0.10567869 0.11075877\n",
      " 0.10400503 0.10211844 0.10572785 0.09034838 0.06419189 0.09918467\n",
      " 0.11449591 0.08852703 0.11093745 0.04688189 0.04856401 0.08838102\n",
      " 0.1103315  0.06143693 0.12124178 0.09320971 0.07262185 0.06633984\n",
      " 0.0859559  0.08960933 0.07955883 0.05649825 0.07086815 0.11243217\n",
      " 0.10081423 0.013525   0.09994207 0.09813671 0.10242453 0.09205656\n",
      " 0.10414581 0.03741384 0.05384734 0.06446719 0.06819545 0.05585334\n",
      " 0.1234565  0.10283951 0.11435939 0.07830856 0.09266902 0.07304979\n",
      " 0.06949927 0.09796443 0.0681267  0.07258319 0.10045939 0.12190453\n",
      " 0.10553765 0.04613676 0.06604269 0.01682554 0.11366941 0.08861798\n",
      " 0.08972458 0.11381479 0.05460975 0.07009787 0.09959118 0.04627176\n",
      " 0.0977566  0.09889433 0.09424274 0.07849234 0.11741041 0.10132781\n",
      " 0.06640002 0.09730337 0.10021899 0.06784452 0.11131068 0.04765344\n",
      " 0.06852256 0.06068707 0.04988706 0.1085807  0.06819205 0.0639645\n",
      " 0.09527697 0.11172513 0.08233459 0.08823442 0.04852571 0.08566944\n",
      " 0.11593604 0.11044001 0.09206506 0.05585064 0.07331368 0.12354229\n",
      " 0.0601448  0.09934627 0.08413915 0.08648204 0.07216555 0.06065389\n",
      " 0.11896515 0.04416706 0.10580004 0.07016072 0.05379339 0.06226537\n",
      " 0.0532264  0.04611299 0.11268694 0.02240057 0.05773869 0.11789243\n",
      " 0.09127612 0.1113556 ]\n",
      "Pruned indices for layer 'block2_conv1': [43, 69, 123]\n",
      "[0.06933067 0.08471052 0.08877518 0.08617761 0.07916486 0.10336622\n",
      " 0.08199854 0.1037138  0.10456463 0.07856888 0.10486199 0.07363839\n",
      " 0.10514227 0.07581104 0.11131526 0.09437769 0.10757751 0.07162369\n",
      " 0.10394572 0.10712303 0.07924627 0.08751768 0.07988261 0.07639641\n",
      " 0.10645177 0.10132507 0.09703518 0.08491158 0.09723588 0.06927119\n",
      " 0.08813339 0.09096693 0.05065786 0.09914368 0.08693182 0.07883112\n",
      " 0.09139876 0.09536494 0.07589099 0.08895697 0.07267554 0.10449854\n",
      " 0.11537091 0.06829615 0.08060271 0.08041497 0.0632864  0.10006742\n",
      " 0.0701323  0.08171844 0.0669587  0.10339688 0.09185211 0.08302139\n",
      " 0.10477766 0.10634632 0.11055114 0.07218724 0.08164759 0.10435944\n",
      " 0.08097445 0.1056138  0.07645086 0.07831921 0.06529541 0.06859739\n",
      " 0.09630183 0.09159354 0.09956678 0.09251836 0.10790595 0.08937676\n",
      " 0.07685573 0.07481025 0.09879144 0.07766307 0.08107495 0.07800724\n",
      " 0.05192797 0.09717653 0.09328261 0.10551076 0.07377524 0.09182328\n",
      " 0.08950452 0.14713506 0.07485121 0.10438969 0.10219552 0.08535005\n",
      " 0.07107449 0.08523416 0.09794939 0.10111981 0.07612252 0.08535751\n",
      " 0.09948087 0.06613394 0.08163032 0.11374367 0.12684274 0.10428032\n",
      " 0.07492431 0.10625286 0.08899152 0.10742585 0.07022561 0.09048326\n",
      " 0.10029281 0.10510457 0.09248579 0.08790068 0.06135263 0.11329498\n",
      " 0.09398703 0.07367011 0.10332626 0.0902457  0.06594407 0.0843994\n",
      " 0.07366204 0.10444411 0.09449199 0.09230502 0.06856152 0.10820372\n",
      " 0.08957279 0.10236027]\n",
      "Pruned indices for layer 'block2_conv2': []\n",
      "[0.04931197 0.04659042 0.08460662 0.03993275 0.04256688 0.04861476\n",
      " 0.044026   0.05024441 0.04043495 0.04497014 0.04150543 0.05109689\n",
      " 0.0527383  0.03637547 0.06166578 0.05519191 0.04097002 0.05761842\n",
      " 0.0433908  0.04702782 0.05096579 0.04642631 0.05022957 0.0499355\n",
      " 0.03630335 0.04778305 0.05326841 0.04600879 0.04096496 0.04703716\n",
      " 0.04648034 0.04494788 0.05622133 0.04909987 0.05126499 0.05480133\n",
      " 0.03648044 0.03561321 0.04434731 0.03979171 0.04633012 0.06265657\n",
      " 0.03965461 0.04769265 0.03163959 0.05137167 0.04614396 0.04029758\n",
      " 0.03970456 0.04032615 0.0521007  0.04704612 0.04378869 0.04358752\n",
      " 0.03954466 0.05435532 0.04840588 0.03631184 0.03096181 0.06203566\n",
      " 0.0494007  0.03724373 0.04979491 0.04629201 0.03520862 0.04550793\n",
      " 0.05162501 0.03576885 0.03975033 0.04565192 0.04882947 0.04011616\n",
      " 0.0453799  0.0427     0.03685864 0.05215402 0.05204169 0.05881669\n",
      " 0.04400748 0.06410917 0.05121148 0.04590391 0.03981358 0.04955646\n",
      " 0.05628698 0.05215794 0.05095003 0.04639336 0.05738181 0.0436778\n",
      " 0.0457872  0.05529518 0.04125053 0.04821173 0.05177569 0.04033913\n",
      " 0.04102831 0.05115309 0.04813123 0.05661305 0.04387024 0.04838532\n",
      " 0.04149044 0.04398362 0.04595061 0.04769999 0.04071571 0.04569526\n",
      " 0.04542758 0.05243777 0.03566237 0.05167279 0.0420451  0.04273642\n",
      " 0.04534288 0.05085913 0.05234876 0.04084799 0.04950828 0.05241555\n",
      " 0.04344848 0.03535095 0.04551003 0.03678805 0.05299866 0.0472798\n",
      " 0.05211984 0.05861433 0.04653468 0.05037726 0.05175869 0.04779578\n",
      " 0.04682246 0.04840689 0.0435376  0.0398423  0.04956648 0.04396955\n",
      " 0.05116592 0.04080246 0.04121564 0.04605782 0.04457668 0.06408875\n",
      " 0.04177807 0.04744744 0.04599913 0.04630934 0.05700903 0.03333143\n",
      " 0.04174794 0.04780933 0.0422454  0.05656844 0.04510296 0.05846971\n",
      " 0.05345481 0.045966   0.04558036 0.04515767 0.05409262 0.05024125\n",
      " 0.05355406 0.04829529 0.0516278  0.03655739 0.05220843 0.04171878\n",
      " 0.05117673 0.04536393 0.05387349 0.03625523 0.04614148 0.04220203\n",
      " 0.04099429 0.04437183 0.05324331 0.05511349 0.04345759 0.04972769\n",
      " 0.04415025 0.0453046  0.0479506  0.05002704 0.06137742 0.04258178\n",
      " 0.03932541 0.04776394 0.04222256 0.05832393 0.03805228 0.04973999\n",
      " 0.04805778 0.05510757 0.04904778 0.04356776 0.04827328 0.05366743\n",
      " 0.04881819 0.03981278 0.05827168 0.04779467 0.04128695 0.04854732\n",
      " 0.04827872 0.0440705  0.05703927 0.05812702 0.05419659 0.04028507\n",
      " 0.05569636 0.05582778 0.04711179 0.04777035 0.05588562 0.04115186\n",
      " 0.05182504 0.04766341 0.0446551  0.04842206 0.04648259 0.05091716\n",
      " 0.06256541 0.05050376 0.05659331 0.04510473 0.04323505 0.04911843\n",
      " 0.04090741 0.0456343  0.05232085 0.05075099 0.04824103 0.03424937\n",
      " 0.04499592 0.04449612 0.06649123 0.04023846 0.04977526 0.05095121\n",
      " 0.04748802 0.04766032 0.04140894 0.0482103  0.05123614 0.0568246\n",
      " 0.04433627 0.04775509 0.05055514 0.05101755 0.04821784 0.04754217\n",
      " 0.06735771 0.04497192 0.06021949 0.03649645]\n",
      "Pruned indices for layer 'block3_conv1': []\n",
      "[0.05869535 0.05088056 0.04636552 0.04336646 0.04569638 0.04721135\n",
      " 0.05833167 0.0495344  0.06204905 0.04847442 0.05752382 0.05373397\n",
      " 0.04176992 0.05693174 0.05820982 0.05035835 0.04287561 0.05702808\n",
      " 0.05900562 0.04319499 0.04407806 0.06116344 0.05197516 0.05814213\n",
      " 0.04285398 0.04538577 0.0503453  0.04296437 0.04290175 0.04279258\n",
      " 0.04680808 0.04237851 0.04187007 0.05369033 0.05213534 0.04928014\n",
      " 0.06023657 0.04854944 0.04774728 0.0422036  0.05026481 0.04245221\n",
      " 0.04536359 0.06337982 0.05057882 0.0537915  0.06730069 0.05102901\n",
      " 0.04817022 0.04849958 0.04556445 0.04402056 0.04853773 0.05789776\n",
      " 0.05374447 0.04479015 0.05551849 0.06536728 0.05157175 0.04612677\n",
      " 0.06469596 0.04828114 0.0487854  0.04554668 0.06983449 0.06284385\n",
      " 0.05031302 0.05147449 0.05388925 0.04287063 0.06512145 0.04687279\n",
      " 0.05476989 0.05059329 0.06508094 0.05440728 0.04449041 0.04844715\n",
      " 0.06296946 0.04052803 0.0506395  0.04726554 0.03720408 0.05670814\n",
      " 0.05053052 0.04724638 0.05654444 0.04409994 0.04694244 0.04852233\n",
      " 0.04310119 0.05114537 0.04072934 0.05804987 0.03540108 0.04913031\n",
      " 0.04614533 0.05540328 0.05294251 0.04774988 0.06429088 0.05234599\n",
      " 0.04649078 0.04594067 0.05126137 0.04664617 0.04102391 0.06423062\n",
      " 0.05760098 0.04271171 0.04177798 0.05618729 0.04813807 0.05927057\n",
      " 0.04662175 0.05118314 0.04837064 0.05999158 0.04978624 0.04956696\n",
      " 0.04534667 0.04666428 0.0422268  0.04841832 0.04408469 0.04401315\n",
      " 0.04744951 0.05499351 0.07474892 0.04166808 0.04131028 0.04603588\n",
      " 0.05393093 0.05447062 0.05373136 0.06090972 0.05443534 0.04829429\n",
      " 0.05152681 0.05918006 0.0377594  0.05234752 0.04833097 0.04915095\n",
      " 0.05437457 0.04885044 0.04139096 0.05877718 0.05239685 0.0570768\n",
      " 0.05388184 0.04422524 0.0472732  0.045305   0.06477433 0.05390068\n",
      " 0.04762129 0.05652875 0.05128173 0.05220411 0.04660157 0.04875037\n",
      " 0.04516182 0.04953785 0.0474322  0.04734891 0.06333473 0.05317086\n",
      " 0.0582942  0.05371374 0.03761774 0.0619821  0.04784757 0.0611974\n",
      " 0.06015592 0.05600752 0.0515844  0.04339396 0.04849361 0.04534467\n",
      " 0.03806286 0.05239394 0.05337361 0.0410348  0.04982647 0.05774277\n",
      " 0.06159956 0.04086391 0.05408332 0.04328065 0.06258637 0.04041667\n",
      " 0.050485   0.04966317 0.04237422 0.04658289 0.05449028 0.04207791\n",
      " 0.05449817 0.06583896 0.04439336 0.05995964 0.05291691 0.05219004\n",
      " 0.04139059 0.04900498 0.05978899 0.04623863 0.04543145 0.04142455\n",
      " 0.04286154 0.05589734 0.05137329 0.05407485 0.0454947  0.04820345\n",
      " 0.04236221 0.04035863 0.05257021 0.05350673 0.04772002 0.04273784\n",
      " 0.05610519 0.05152731 0.05620088 0.04267927 0.05350863 0.04885915\n",
      " 0.04140975 0.06667987 0.05855738 0.0455701  0.05636821 0.05604266\n",
      " 0.04930167 0.04797154 0.05944255 0.03730108 0.0476945  0.04866424\n",
      " 0.05061064 0.04598563 0.05469979 0.04704611 0.04364348 0.04260706\n",
      " 0.05193483 0.04885612 0.03539203 0.04216138 0.06130162 0.04388071\n",
      " 0.05695945 0.04627664 0.04591873 0.05018452]\n",
      "Pruned indices for layer 'block3_conv2': []\n",
      "[0.05083849 0.04518712 0.05578544 0.04536645 0.04502627 0.04248234\n",
      " 0.04701383 0.0471501  0.04786834 0.04883176 0.04624121 0.04859777\n",
      " 0.04640001 0.05203887 0.04823082 0.05977412 0.05066212 0.05076389\n",
      " 0.04993476 0.05069948 0.05634591 0.04368866 0.06193588 0.06400269\n",
      " 0.04577118 0.04594705 0.0606235  0.05582853 0.05276673 0.05611897\n",
      " 0.04865362 0.04379033 0.0613086  0.04896723 0.05395486 0.05139469\n",
      " 0.04908631 0.06239137 0.05781869 0.04285784 0.04991028 0.04339867\n",
      " 0.05194747 0.03907668 0.05452973 0.05477342 0.05132731 0.04018495\n",
      " 0.05225647 0.0515541  0.05661363 0.05758152 0.04720122 0.0460018\n",
      " 0.05549662 0.04596633 0.05191053 0.04791119 0.04219013 0.04725103\n",
      " 0.05400029 0.05384663 0.04778755 0.06676912 0.05004464 0.04625946\n",
      " 0.05875692 0.04647593 0.03932306 0.05664167 0.04788101 0.04177496\n",
      " 0.03984445 0.04391849 0.05154722 0.05128838 0.05496444 0.06094446\n",
      " 0.0611004  0.04725111 0.05443764 0.03806152 0.05557647 0.04828992\n",
      " 0.05421138 0.05044893 0.05405842 0.04736323 0.04570685 0.04926972\n",
      " 0.05215915 0.0452484  0.05350258 0.05398359 0.04627658 0.05377066\n",
      " 0.04633638 0.04504947 0.04974676 0.04922281 0.05258397 0.05451606\n",
      " 0.04799794 0.03536225 0.04493032 0.05891876 0.05212336 0.04368645\n",
      " 0.04916584 0.05019683 0.05495648 0.04672937 0.04808787 0.04541185\n",
      " 0.04379069 0.05310767 0.05731813 0.05689099 0.04718327 0.05015904\n",
      " 0.05080351 0.05736893 0.04844841 0.04818935 0.04642553 0.04561648\n",
      " 0.04977826 0.04724292 0.06524788 0.0484896  0.05787487 0.04376685\n",
      " 0.05135146 0.05712349 0.04691244 0.0579589  0.05872936 0.05768304\n",
      " 0.05187909 0.050944   0.05686015 0.05119275 0.04025976 0.05403957\n",
      " 0.04934904 0.05069754 0.0551195  0.0460548  0.04387554 0.05141249\n",
      " 0.05041679 0.04469615 0.05488466 0.06020026 0.05365976 0.04326296\n",
      " 0.04892351 0.0551973  0.04615397 0.05126915 0.04062004 0.05079656\n",
      " 0.05134634 0.051626   0.04380219 0.05419243 0.04840771 0.05005714\n",
      " 0.04879526 0.04414786 0.04638175 0.05432656 0.05729678 0.04829524\n",
      " 0.0657065  0.05644308 0.06965272 0.05383135 0.03952796 0.04648847\n",
      " 0.05660858 0.04873697 0.04012635 0.04391102 0.05503458 0.04852179\n",
      " 0.06257552 0.05272239 0.05396457 0.04842824 0.04756383 0.04375425\n",
      " 0.04448223 0.05109074 0.04749973 0.04985855 0.05206166 0.0425623\n",
      " 0.04300404 0.05421515 0.04481104 0.06037959 0.0442542  0.05427283\n",
      " 0.06041063 0.04940024 0.04485787 0.04815169 0.0438863  0.03802563\n",
      " 0.05441302 0.04536836 0.0481876  0.04998858 0.04030745 0.06335866\n",
      " 0.05090505 0.05220957 0.04879767 0.05416208 0.05257654 0.04413498\n",
      " 0.05111379 0.04809054 0.05078609 0.04243558 0.05240017 0.04942048\n",
      " 0.04782771 0.05853517 0.05024733 0.06778719 0.05264127 0.0488106\n",
      " 0.04953092 0.05372068 0.05474762 0.04047259 0.04851103 0.05806315\n",
      " 0.05081227 0.05304364 0.04618435 0.04665346 0.05669947 0.05497829\n",
      " 0.05248092 0.04423213 0.04534516 0.05072743 0.05076545 0.06013432\n",
      " 0.04163179 0.05158438 0.04530098 0.04874083]\n",
      "Pruned indices for layer 'block3_conv3': []\n",
      "[0.02790566 0.02463656 0.02715654 0.02563121 0.02154701 0.02632372\n",
      " 0.02566573 0.02877707 0.02335169 0.02313971 0.02542039 0.02732782\n",
      " 0.02955401 0.02604439 0.02572069 0.02409368 0.02466068 0.02688179\n",
      " 0.03044007 0.02451021 0.02653666 0.02673449 0.02977748 0.03077956\n",
      " 0.02604989 0.02665302 0.02345154 0.02325162 0.02829793 0.02758824\n",
      " 0.0303257  0.024216   0.02252197 0.02496807 0.02445714 0.02638798\n",
      " 0.02767292 0.02332178 0.02307553 0.02554228 0.02599764 0.02741028\n",
      " 0.02525805 0.02752711 0.02813348 0.0254153  0.02368732 0.02744608\n",
      " 0.02545187 0.02788725 0.02533505 0.02915528 0.03013389 0.02697234\n",
      " 0.02465811 0.02542711 0.03082317 0.02447889 0.02600351 0.02547203\n",
      " 0.03150108 0.03744484 0.02802781 0.02826447 0.02434643 0.02452564\n",
      " 0.03159552 0.02851025 0.02488798 0.02585596 0.0266436  0.02863496\n",
      " 0.0318019  0.02255134 0.03274156 0.02752539 0.02256456 0.0246581\n",
      " 0.02860085 0.02957487 0.02455004 0.02229862 0.02954196 0.02357428\n",
      " 0.02903637 0.02486407 0.03114277 0.03107682 0.02472792 0.0272575\n",
      " 0.03695872 0.02495166 0.02564351 0.02395124 0.02541903 0.02486474\n",
      " 0.02612016 0.023082   0.02439927 0.0277414  0.02372159 0.02708473\n",
      " 0.02685825 0.02803082 0.02457071 0.03078526 0.02482111 0.02814792\n",
      " 0.02720091 0.02522407 0.02395125 0.02583157 0.02870097 0.02324592\n",
      " 0.03146774 0.02707426 0.02659742 0.03501238 0.02611369 0.02452274\n",
      " 0.02353103 0.02375962 0.02524246 0.02584454 0.02986633 0.02394181\n",
      " 0.02940221 0.03176997 0.02758986 0.02760589 0.0269976  0.02404812\n",
      " 0.02705145 0.02777782 0.02419381 0.02623429 0.02412236 0.0233074\n",
      " 0.0238702  0.02485972 0.02435357 0.02857656 0.02889112 0.0267192\n",
      " 0.02777018 0.02884174 0.0285491  0.0282124  0.02963306 0.02682883\n",
      " 0.0244499  0.02431412 0.02823193 0.02831477 0.02374371 0.02740247\n",
      " 0.02463834 0.02989455 0.02538615 0.02216611 0.02412827 0.0279621\n",
      " 0.02814327 0.0255708  0.02201647 0.02475366 0.02979125 0.02304477\n",
      " 0.02479301 0.03116924 0.02613576 0.0235638  0.02556116 0.02782254\n",
      " 0.02460468 0.02507389 0.02378824 0.02193537 0.02909336 0.02413501\n",
      " 0.02796871 0.02521618 0.02268659 0.0257908  0.02380094 0.02645008\n",
      " 0.0242789  0.02742964 0.03198474 0.02544494 0.0282336  0.02832567\n",
      " 0.02520485 0.02462661 0.02627026 0.02830161 0.02283377 0.02457655\n",
      " 0.02238155 0.02797102 0.02412031 0.02668677 0.02517923 0.031436\n",
      " 0.02613413 0.02449501 0.02511171 0.02342668 0.02795567 0.02851772\n",
      " 0.03193765 0.02811433 0.0270634  0.02482398 0.02647238 0.02335817\n",
      " 0.02779583 0.02108455 0.02248334 0.02307378 0.0297436  0.02487561\n",
      " 0.02729702 0.02842755 0.03004846 0.02338336 0.03078532 0.02778074\n",
      " 0.02505383 0.02496045 0.02907886 0.02500224 0.02881414 0.02350512\n",
      " 0.02740612 0.02826169 0.02475368 0.02868231 0.02536056 0.02875356\n",
      " 0.02939783 0.03807916 0.02752705 0.02560916 0.0292889  0.02641385\n",
      " 0.02409601 0.0231094  0.0299577  0.02774417 0.02911392 0.03219107\n",
      " 0.02551004 0.02249374 0.02533498 0.02271665 0.02368857 0.03250283\n",
      " 0.03074292 0.02727814 0.02568896 0.02218111 0.02614789 0.02577308\n",
      " 0.02684632 0.0242582  0.0270268  0.02624053 0.03122157 0.02573142\n",
      " 0.02810572 0.02425547 0.02635606 0.02559479 0.02452178 0.02594274\n",
      " 0.02535296 0.02431029 0.02639243 0.03047281 0.02707654 0.02693251\n",
      " 0.027865   0.02727183 0.02615227 0.02654393 0.02544977 0.02755371\n",
      " 0.02709465 0.02787923 0.02687307 0.02742314 0.02827211 0.02799111\n",
      " 0.02486969 0.02467518 0.02966965 0.02557921 0.02350779 0.02857777\n",
      " 0.02616756 0.02849781 0.02490053 0.02691463 0.02654019 0.02695749\n",
      " 0.02369655 0.02499704 0.0289416  0.0267886  0.02712899 0.02434222\n",
      " 0.02739326 0.02585981 0.02604345 0.02787856 0.02433624 0.02913681\n",
      " 0.02507169 0.0261247  0.02507511 0.03020535 0.0273431  0.0226052\n",
      " 0.02494494 0.02994895 0.02490254 0.02774728 0.02308178 0.02517932\n",
      " 0.02586827 0.02422501 0.02441541 0.02329564 0.02680639 0.02283899\n",
      " 0.02801353 0.03017694 0.02704168 0.02380469 0.0240453  0.02996903\n",
      " 0.03048625 0.02825314 0.02776035 0.03112894 0.02661842 0.02926748\n",
      " 0.02666917 0.02537025 0.02672165 0.02351669 0.0384985  0.0246141\n",
      " 0.02898307 0.02330961 0.02641038 0.02811577 0.02367698 0.02604201\n",
      " 0.02360085 0.02373203 0.02469462 0.02434646 0.02361069 0.02457538\n",
      " 0.02475115 0.03159421 0.02731252 0.02658824 0.02672991 0.02544343\n",
      " 0.02707228 0.02752731 0.02591454 0.02614051 0.02689386 0.02390994\n",
      " 0.02773928 0.02818498 0.02598889 0.0292381  0.02235814 0.02813041\n",
      " 0.02678423 0.02679165 0.03948513 0.0258991  0.02301447 0.03617708\n",
      " 0.02807313 0.02484101 0.02396942 0.03064752 0.02786295 0.02567778\n",
      " 0.02661858 0.02420656 0.02632152 0.02846706 0.02671262 0.02993315\n",
      " 0.02555165 0.0258382  0.02478461 0.02586639 0.0245351  0.02904656\n",
      " 0.02994715 0.0277073  0.02440372 0.02857882 0.03084594 0.02886837\n",
      " 0.02950819 0.02884779 0.0261571  0.029002   0.0259015  0.0239222\n",
      " 0.02441309 0.02816807 0.02561465 0.02710142 0.02480378 0.02805277\n",
      " 0.02291398 0.02795149 0.02982719 0.02831302 0.02166754 0.02923325\n",
      " 0.022012   0.02893715 0.02538705 0.02558989 0.02411134 0.02631495\n",
      " 0.02084752 0.02771617 0.02472394 0.03246966 0.02530192 0.03090151\n",
      " 0.02845955 0.03345965 0.02660633 0.02783901 0.02498153 0.02573263\n",
      " 0.02828478 0.02576271 0.03065499 0.02756301 0.02598372 0.02576352\n",
      " 0.02463183 0.0261843  0.0238422  0.02317431 0.02207613 0.03187091\n",
      " 0.02878441 0.02629805 0.0272754  0.02422211 0.03120373 0.03524346\n",
      " 0.02398899 0.02119129 0.02509106 0.02251566 0.02782083 0.02594201\n",
      " 0.02734237 0.02598083 0.02793026 0.02764217 0.02341957 0.02704932\n",
      " 0.0233264  0.02370589 0.02442348 0.02898273 0.02780942 0.03096332\n",
      " 0.03009832 0.02823242 0.02444541 0.02536613 0.02825963 0.02882754\n",
      " 0.0274848  0.02471968 0.02467334 0.02557007 0.02481955 0.0260065\n",
      " 0.02536353 0.02828237 0.02899465 0.02356521 0.02746756 0.02579525\n",
      " 0.02555723 0.02647926 0.0241842  0.02778552 0.02475653 0.02388398\n",
      " 0.0262602  0.0307164 ]\n",
      "Pruned indices for layer 'block4_conv1': [1, 3, 4, 6, 8, 9, 10, 14, 15, 16, 19, 26, 27, 31, 32, 33, 34, 37, 38, 39, 40, 42, 45, 46, 48, 50, 54, 55, 57, 59, 64, 65, 68, 69, 73, 76, 77, 80, 81, 83, 85, 88, 91, 92, 93, 94, 95, 97, 98, 100, 104, 106, 109, 110, 111, 113, 119, 120, 121, 122, 123, 125, 131, 134, 136, 137, 138, 139, 140, 150, 151, 154, 156, 158, 159, 160, 163, 164, 165, 167, 168, 171, 172, 174, 175, 176, 177, 179, 181, 182, 183, 184, 186, 189, 192, 193, 196, 197, 198, 200, 202, 205, 206, 207, 213, 215, 217, 218, 219, 221, 225, 228, 229, 231, 233, 236, 238, 243, 246, 247, 252, 253, 254, 255, 256, 260, 261, 263, 265, 269, 271, 273, 274, 275, 276, 277, 286, 294, 295, 297, 298, 302, 306, 307, 311, 313, 316, 318, 320, 323, 324, 326, 328, 329, 330, 331, 332, 333, 335, 339, 340, 349, 351, 353, 355, 358, 360, 361, 362, 363, 364, 365, 366, 371, 374, 377, 380, 382, 387, 388, 391, 392, 395, 397, 402, 403, 404, 405, 406, 410, 418, 419, 420, 422, 424, 426, 430, 432, 434, 435, 436, 438, 440, 442, 448, 449, 451, 454, 455, 456, 458, 459, 460, 465, 468, 469, 470, 471, 473, 475, 478, 480, 481, 482, 488, 489, 493, 494, 495, 496, 498, 501, 503, 504, 506, 508, 509]\n",
      "[0.03079838 0.02857999 0.0276606  0.02911293 0.03140653 0.029453\n",
      " 0.02367026 0.02691012 0.02509569 0.02696095 0.02489481 0.02458454\n",
      " 0.03138336 0.02576083 0.0347621  0.02960356 0.03610459 0.02958727\n",
      " 0.02688112 0.0279404  0.02649637 0.02632716 0.02753082 0.03035861\n",
      " 0.02897974 0.0252932  0.03064631 0.02500015 0.02842777 0.02826378\n",
      " 0.02558029 0.02462732 0.02647251 0.0275067  0.03025037 0.02910861\n",
      " 0.03038561 0.02836028 0.03140256 0.02990685 0.02357361 0.02749569\n",
      " 0.02658649 0.02921148 0.02877174 0.02699134 0.02651849 0.03223222\n",
      " 0.0285646  0.0264523  0.02792696 0.03145677 0.02849775 0.02633116\n",
      " 0.02751674 0.03214151 0.027152   0.02744788 0.02837765 0.03289616\n",
      " 0.02659087 0.02621322 0.03005362 0.03027208 0.02499372 0.02738554\n",
      " 0.02695458 0.02860232 0.02976375 0.02623788 0.02975726 0.02603915\n",
      " 0.02513242 0.02813554 0.02527837 0.0251078  0.02903911 0.02872824\n",
      " 0.02702917 0.02772133 0.02926916 0.03138572 0.02528977 0.02677112\n",
      " 0.02623611 0.02974661 0.0266045  0.02999753 0.03169002 0.02994214\n",
      " 0.03001186 0.02846547 0.02958183 0.02920935 0.02481481 0.02718648\n",
      " 0.02712247 0.02894914 0.02946863 0.0290274  0.02881908 0.02886214\n",
      " 0.02797803 0.02697859 0.032977   0.02673822 0.02669134 0.03224748\n",
      " 0.033315   0.02570055 0.02930199 0.02452917 0.02766738 0.02873967\n",
      " 0.03029085 0.03038227 0.03081889 0.02833935 0.02745386 0.02871504\n",
      " 0.03140723 0.0288681  0.02887022 0.02675511 0.02619549 0.02994185\n",
      " 0.02777164 0.0288573  0.02580025 0.02931724 0.02638661 0.02763405\n",
      " 0.03122775 0.02579142 0.02799025 0.0287777  0.02812893 0.02954898\n",
      " 0.02637407 0.02796795 0.02648618 0.02723303 0.02626028 0.03064881\n",
      " 0.02903275 0.02671827 0.02604836 0.02974729 0.02790305 0.02574628\n",
      " 0.03035364 0.03020458 0.02657182 0.02456214 0.02839258 0.0317218\n",
      " 0.03053821 0.02516103 0.02850557 0.02838315 0.03083405 0.02808511\n",
      " 0.0260711  0.02902459 0.02840416 0.02831299 0.02652436 0.02777419\n",
      " 0.02487497 0.0313457  0.02688229 0.03066982 0.02492195 0.02511381\n",
      " 0.02971259 0.02951263 0.02533461 0.03154236 0.02556864 0.0291903\n",
      " 0.02632545 0.03165429 0.02727284 0.02922556 0.02956454 0.02871885\n",
      " 0.02793311 0.02612724 0.02514157 0.02855709 0.0289077  0.02649048\n",
      " 0.02642811 0.02841214 0.02501206 0.02675039 0.02849379 0.02814162\n",
      " 0.02855638 0.02941274 0.02795605 0.03033231 0.03207671 0.03024394\n",
      " 0.02871268 0.02419945 0.02845477 0.03109677 0.02424897 0.02644225\n",
      " 0.03035103 0.02374303 0.03094594 0.02896229 0.02913881 0.02894087\n",
      " 0.02812291 0.02555631 0.03955455 0.02849271 0.02697526 0.02582937\n",
      " 0.0259441  0.02679777 0.02709253 0.02729723 0.03647533 0.02908703\n",
      " 0.02960937 0.02567384 0.02923663 0.02650968 0.02767282 0.0270075\n",
      " 0.02588603 0.03118528 0.02598629 0.02934795 0.02877934 0.02723792\n",
      " 0.02786428 0.0268173  0.03555383 0.03828413 0.02675699 0.0250052\n",
      " 0.02427099 0.02909656 0.02914809 0.02628098 0.02808815 0.02585888\n",
      " 0.02599354 0.02858896 0.02486745 0.02500713 0.02784759 0.02676532\n",
      " 0.0329949  0.02866401 0.03076361 0.02963089 0.02755999 0.02792524\n",
      " 0.027031   0.02697218 0.02834984 0.02834421 0.02741334 0.02891037\n",
      " 0.02836885 0.03349382 0.02788037 0.02868957 0.02543333 0.02828132\n",
      " 0.02864028 0.02790671 0.02755751 0.02874285 0.03137309 0.02444646\n",
      " 0.0281861  0.0244797  0.02595356 0.02950739 0.02640841 0.02769418\n",
      " 0.02947548 0.02622653 0.02748697 0.02552585 0.0342036  0.02372471\n",
      " 0.02371071 0.03070381 0.02865377 0.02677969 0.02932966 0.0251472\n",
      " 0.02722713 0.02674047 0.02614972 0.03015378 0.02669541 0.02791773\n",
      " 0.02926706 0.02489408 0.03026688 0.03000186 0.02782184 0.02580617\n",
      " 0.02286723 0.02969521 0.02800941 0.02689721 0.02867733 0.02796265\n",
      " 0.03003675 0.03139324 0.02867984 0.03029526 0.02516487 0.02864631\n",
      " 0.03072085 0.02558873 0.02824126 0.02668142 0.02419535 0.02501916\n",
      " 0.02519261 0.03190156 0.02722995 0.02659828 0.03078349 0.0261081\n",
      " 0.02711937 0.02688576 0.02529876 0.02875011 0.02603318 0.02617769\n",
      " 0.02659557 0.02857945 0.02811037 0.02547593 0.0300286  0.02862839\n",
      " 0.0296606  0.02968009 0.03592746 0.02653934 0.02839426 0.02660981\n",
      " 0.03357594 0.03064747 0.02800681 0.0259782  0.0263908  0.02896112\n",
      " 0.03021083 0.02610596 0.02694719 0.02628694 0.03153649 0.02759581\n",
      " 0.02623483 0.02866946 0.02782228 0.02964172 0.02441633 0.02750994\n",
      " 0.02535628 0.03137021 0.02937002 0.03057831 0.02641745 0.0264356\n",
      " 0.02599966 0.0237741  0.02780118 0.02492998 0.02457765 0.02664847\n",
      " 0.02811848 0.02967781 0.02687512 0.0258545  0.02822858 0.02858788\n",
      " 0.0282604  0.02531739 0.03446656 0.0256202  0.03201669 0.02643544\n",
      " 0.02571021 0.02561976 0.02864923 0.02670411 0.02632088 0.02934833\n",
      " 0.02602667 0.02584533 0.0342003  0.03003586 0.03522735 0.02871312\n",
      " 0.02776417 0.02923021 0.02848063 0.02618095 0.02837108 0.02658419\n",
      " 0.02767946 0.0282029  0.02512743 0.02656189 0.03016441 0.03101955\n",
      " 0.02751993 0.02845033 0.02736313 0.02501009 0.02670079 0.03360468\n",
      " 0.02276999 0.03085259 0.02861644 0.02891668 0.02758542 0.02813942\n",
      " 0.029101   0.02526597 0.02559415 0.02596301 0.02692965 0.0275415\n",
      " 0.02626847 0.02463929 0.02975175 0.02694069 0.02848439 0.02622832\n",
      " 0.0241297  0.02683524 0.02783447 0.02813623 0.02716051 0.0291612\n",
      " 0.02673166 0.02843097 0.02433847 0.0306609  0.02969088 0.0257115\n",
      " 0.02811526 0.03063624 0.02772346 0.02938316 0.02654723 0.0271572\n",
      " 0.02775025 0.02253518 0.03005998 0.02695106 0.0282878  0.02687828\n",
      " 0.02393157 0.02776808 0.02728485 0.03217901 0.03071223 0.02485506\n",
      " 0.02914879 0.02630002 0.02849294 0.0285901  0.02719518 0.02577199\n",
      " 0.02641474 0.02476733 0.02537642 0.03155561 0.02693385 0.02784119\n",
      " 0.03067448 0.02907627 0.02843242 0.03013594 0.02917956 0.03137735\n",
      " 0.02743883 0.02442896 0.02707397 0.02356886 0.02601906 0.03094735\n",
      " 0.02899366 0.02413501 0.03223043 0.02720637 0.02505975 0.02834535\n",
      " 0.0261831  0.02070225 0.03162763 0.02599227 0.02889841 0.02901318\n",
      " 0.03209371 0.02708051]\n",
      "Pruned indices for layer 'block4_conv2': [6, 8, 10, 11, 13, 25, 27, 30, 31, 40, 64, 72, 74, 75, 82, 94, 109, 111, 128, 133, 149, 153, 157, 168, 172, 173, 176, 178, 188, 194, 205, 208, 211, 217, 221, 222, 229, 234, 236, 245, 246, 251, 252, 254, 255, 274, 281, 283, 284, 291, 293, 294, 299, 307, 311, 312, 322, 325, 328, 329, 330, 338, 345, 357, 370, 372, 378, 379, 381, 382, 387, 391, 393, 396, 397, 403, 416, 423, 426, 433, 434, 435, 439, 444, 452, 455, 463, 468, 473, 479, 481, 482, 493, 495, 499, 502, 505, 507]\n",
      "[0.02564985 0.02954755 0.02637612 0.03445879 0.02664133 0.02751818\n",
      " 0.02550989 0.02851931 0.0324366  0.02588497 0.02801895 0.02757729\n",
      " 0.02641011 0.02670285 0.02866922 0.02732141 0.02726073 0.02842707\n",
      " 0.02552008 0.02755525 0.03004139 0.02618467 0.0275303  0.02936501\n",
      " 0.02711773 0.02968699 0.0276525  0.02569855 0.02753317 0.02324519\n",
      " 0.02744603 0.02521826 0.02526672 0.0277934  0.033144   0.02509992\n",
      " 0.02595458 0.02598345 0.03495824 0.02913328 0.03680775 0.02946266\n",
      " 0.03148016 0.03000632 0.02672208 0.02830466 0.02580422 0.02589696\n",
      " 0.02737612 0.03071275 0.03207628 0.02611944 0.02966524 0.02926836\n",
      " 0.02928926 0.02812641 0.02566936 0.02762067 0.02537139 0.02559568\n",
      " 0.02566257 0.02617185 0.02755606 0.02739383 0.02769615 0.03047048\n",
      " 0.02835667 0.02578629 0.03150428 0.0316692  0.02669207 0.02657706\n",
      " 0.02601691 0.02880025 0.02855131 0.025252   0.02807659 0.02767826\n",
      " 0.02471109 0.02488004 0.03409756 0.02798507 0.03589743 0.02484033\n",
      " 0.02869755 0.03334182 0.02899621 0.02628973 0.02894638 0.02599064\n",
      " 0.02592092 0.02947265 0.02787229 0.03037651 0.02935735 0.03002346\n",
      " 0.03014184 0.02713832 0.02603876 0.0311053  0.02476706 0.02907655\n",
      " 0.02852685 0.02884563 0.02898027 0.02925512 0.02778521 0.02504542\n",
      " 0.02888743 0.03458883 0.0284826  0.02972006 0.02811968 0.02641222\n",
      " 0.03337139 0.02690794 0.03141831 0.02426996 0.02981959 0.02800243\n",
      " 0.02933186 0.02706355 0.02717035 0.02837682 0.02972701 0.02828261\n",
      " 0.02496366 0.02437158 0.03308153 0.02446909 0.02609811 0.02726227\n",
      " 0.02795733 0.0241865  0.02691052 0.02837596 0.02967788 0.02722493\n",
      " 0.0246184  0.02626018 0.03017909 0.02620623 0.02718095 0.03556593\n",
      " 0.02947522 0.02852299 0.02821242 0.02891329 0.02945626 0.03250008\n",
      " 0.02724735 0.02594808 0.02620845 0.02789698 0.02777179 0.02373945\n",
      " 0.02511391 0.02738481 0.0276648  0.02841155 0.02578249 0.02563556\n",
      " 0.02862981 0.02928737 0.0269748  0.02770059 0.03090604 0.02651673\n",
      " 0.02858742 0.02848918 0.02755074 0.0279851  0.03267593 0.02562862\n",
      " 0.02792269 0.02905751 0.03099393 0.02864768 0.02875885 0.02760492\n",
      " 0.02613812 0.02458747 0.0314853  0.0237393  0.02925842 0.02881717\n",
      " 0.0254992  0.02619096 0.02612706 0.02589554 0.02778389 0.02872027\n",
      " 0.03205827 0.02612898 0.03263141 0.03013665 0.03118457 0.03065536\n",
      " 0.02910557 0.0235699  0.03122622 0.02986166 0.02771422 0.02735609\n",
      " 0.02649123 0.02789714 0.02988072 0.02543096 0.03933444 0.03107999\n",
      " 0.02561006 0.02876465 0.02692378 0.03041243 0.02642675 0.02633988\n",
      " 0.02829887 0.02704693 0.02685535 0.03036785 0.02696402 0.03494129\n",
      " 0.03490561 0.02593427 0.02943043 0.03149198 0.02512129 0.02882272\n",
      " 0.0269102  0.02437188 0.02640983 0.02816731 0.03076403 0.03481488\n",
      " 0.02786799 0.02620181 0.02872008 0.02752841 0.02797708 0.02724261\n",
      " 0.02907809 0.02934019 0.0253333  0.02810268 0.02645945 0.02618266\n",
      " 0.02738514 0.02734825 0.0292168  0.0280316  0.02480691 0.03202587\n",
      " 0.02878851 0.02410514 0.02816762 0.02641543 0.02797063 0.03015779\n",
      " 0.02937747 0.02676264 0.0294455  0.02450909 0.02814913 0.02895428\n",
      " 0.02805701 0.02620425 0.02739204 0.02327917 0.02582086 0.02584214\n",
      " 0.03227265 0.02944238 0.03343692 0.02880492 0.02905707 0.02727786\n",
      " 0.03039767 0.03129388 0.02581988 0.02759016 0.02755578 0.02775375\n",
      " 0.029185   0.02394975 0.02834975 0.02663578 0.02788933 0.0285558\n",
      " 0.02731208 0.02680431 0.02627839 0.0279496  0.0289874  0.02802761\n",
      " 0.02860556 0.03028624 0.02655876 0.03005734 0.03055731 0.0297988\n",
      " 0.02629652 0.02969622 0.02792464 0.0286997  0.02538582 0.02299367\n",
      " 0.02904447 0.03168247 0.02667321 0.0311559  0.02764015 0.02723499\n",
      " 0.02653222 0.02747348 0.02636957 0.02696554 0.02825939 0.02603795\n",
      " 0.02556399 0.02727553 0.02807401 0.02676088 0.02581609 0.02488371\n",
      " 0.02830063 0.02707445 0.02486896 0.02824164 0.0281236  0.0269723\n",
      " 0.02838659 0.02929613 0.04153071 0.02512742 0.02859176 0.02913139\n",
      " 0.02676791 0.02486343 0.02707331 0.02613414 0.0255771  0.03034865\n",
      " 0.02609348 0.02309856 0.02570119 0.0282133  0.03065405 0.02505337\n",
      " 0.02479295 0.0295661  0.0289629  0.02898208 0.02526746 0.02934307\n",
      " 0.02798054 0.02822375 0.03008734 0.02836442 0.02810001 0.03260732\n",
      " 0.02562349 0.02752323 0.02790669 0.03200489 0.02600181 0.02860525\n",
      " 0.02928821 0.02918549 0.02638477 0.02862069 0.02712027 0.02994815\n",
      " 0.02857042 0.03036897 0.02575699 0.02526092 0.02893356 0.02509457\n",
      " 0.02994553 0.02922731 0.02906084 0.02876743 0.02612038 0.02488244\n",
      " 0.02626598 0.02616961 0.02670652 0.02609396 0.03166203 0.0310043\n",
      " 0.0307301  0.03055476 0.02652139 0.0256616  0.02791195 0.02742829\n",
      " 0.02336936 0.03173082 0.02669758 0.02949174 0.02701954 0.02405917\n",
      " 0.02711315 0.02575293 0.02705821 0.02855801 0.0283861  0.02997328\n",
      " 0.02942022 0.02795425 0.02727646 0.02323154 0.02577875 0.02540776\n",
      " 0.02864468 0.02417593 0.02723556 0.02588623 0.02726748 0.02611515\n",
      " 0.03027963 0.02475185 0.02273964 0.02544801 0.02874925 0.02654623\n",
      " 0.03398092 0.02674655 0.02862429 0.02788814 0.02728877 0.0279836\n",
      " 0.0275728  0.02903825 0.02827656 0.0279109  0.02680317 0.02728282\n",
      " 0.02745314 0.02638165 0.0287387  0.02843129 0.02796925 0.03026759\n",
      " 0.02664744 0.03062934 0.02526226 0.03363998 0.02855252 0.02875193\n",
      " 0.02643035 0.02612696 0.03195835 0.02958835 0.02763531 0.0289234\n",
      " 0.0294459  0.0276332  0.02910163 0.02951074 0.02799065 0.03021313\n",
      " 0.03044781 0.02916477 0.0262937  0.03030916 0.02752483 0.0326617\n",
      " 0.03080298 0.02826921 0.03280716 0.02655532 0.03041851 0.02764204\n",
      " 0.02922448 0.02751568 0.02519002 0.02836019 0.02437428 0.02861512\n",
      " 0.02775711 0.02610113 0.0293533  0.02568511 0.02704986 0.02812901\n",
      " 0.02696393 0.03003889 0.02729806 0.02748017 0.02756202 0.03038789\n",
      " 0.02605439 0.02620748 0.02793201 0.0263854  0.02992796 0.03129481\n",
      " 0.02548807 0.02700504 0.02848997 0.02309629 0.02544113 0.02605922\n",
      " 0.02504934 0.02693344 0.02543061 0.02869943 0.02808456 0.03344301\n",
      " 0.02415122 0.02498513]\n",
      "Pruned indices for layer 'block4_conv3': [0, 6, 9, 18, 27, 29, 31, 32, 35, 36, 37, 46, 47, 56, 58, 59, 60, 67, 75, 78, 79, 83, 89, 90, 100, 107, 117, 126, 127, 129, 133, 138, 151, 155, 156, 160, 161, 173, 181, 183, 186, 189, 199, 207, 210, 223, 226, 229, 242, 250, 253, 261, 267, 268, 269, 278, 283, 304, 305, 318, 322, 323, 326, 333, 337, 340, 343, 344, 347, 348, 352, 360, 374, 375, 377, 383, 393, 396, 401, 403, 411, 412, 413, 415, 417, 421, 422, 423, 446, 476, 478, 483, 498, 501, 502, 504, 506, 510, 511]\n",
      "[0.02810562 0.02397    0.02729474 0.02633082 0.02708001 0.02673407\n",
      " 0.02679767 0.02723143 0.02766239 0.02411633 0.02739921 0.02622272\n",
      " 0.02895463 0.02508575 0.02708785 0.02637073 0.02673122 0.02643584\n",
      " 0.02726247 0.02722079 0.02576744 0.02970173 0.02724393 0.032188\n",
      " 0.02904362 0.02689556 0.03045238 0.02812346 0.03001411 0.02804381\n",
      " 0.02819358 0.02551625 0.02812089 0.02828918 0.02726923 0.02767733\n",
      " 0.02683395 0.03227618 0.0293164  0.02979081 0.02817074 0.03207893\n",
      " 0.0325641  0.02528451 0.02720388 0.02729262 0.02631286 0.02470485\n",
      " 0.02925676 0.02810934 0.02579932 0.02798308 0.03008054 0.0262466\n",
      " 0.03289067 0.03012028 0.02986594 0.02973847 0.02641538 0.02733194\n",
      " 0.02762342 0.02689839 0.0280277  0.02438377 0.0276503  0.02783761\n",
      " 0.027163   0.02725069 0.02676302 0.02694012 0.02909023 0.02763707\n",
      " 0.02774826 0.02878384 0.02509763 0.02866833 0.02718927 0.02712272\n",
      " 0.02881081 0.02745968 0.03046306 0.02905972 0.02652107 0.02790977\n",
      " 0.03050108 0.02616396 0.02864996 0.02664323 0.03300033 0.02878709\n",
      " 0.02396758 0.02652176 0.02812406 0.02864074 0.02945511 0.02685885\n",
      " 0.02682313 0.02769261 0.03047686 0.02812023 0.02684321 0.02833827\n",
      " 0.02759119 0.02803377 0.03846826 0.03349076 0.02926797 0.02795217\n",
      " 0.03173934 0.02661451 0.02832012 0.02921878 0.03015919 0.02920322\n",
      " 0.02960762 0.02673797 0.0258611  0.028168   0.02794844 0.02695076\n",
      " 0.02826506 0.02794328 0.02874696 0.02725712 0.025653   0.02793896\n",
      " 0.02705624 0.02803652 0.02820175 0.02979423 0.02837238 0.02603419\n",
      " 0.02717341 0.02757699 0.0268524  0.02751098 0.02763296 0.02770966\n",
      " 0.02545077 0.02583333 0.02763936 0.0258808  0.0279479  0.02791481\n",
      " 0.02858928 0.02862008 0.02884866 0.02803084 0.02735593 0.02854944\n",
      " 0.02835661 0.02669314 0.02774272 0.02634749 0.02767818 0.02785537\n",
      " 0.029862   0.02727572 0.02870594 0.02630914 0.02869882 0.02707848\n",
      " 0.02666546 0.02703899 0.02863677 0.02694803 0.02905486 0.02774311\n",
      " 0.02899469 0.03181262 0.03177987 0.02971377 0.02796328 0.02686889\n",
      " 0.02880371 0.02775496 0.03088404 0.02598226 0.02978878 0.02755059\n",
      " 0.02712197 0.02675652 0.02668874 0.03025744 0.02644291 0.02767486\n",
      " 0.02835534 0.02680025 0.02626265 0.02907861 0.02757674 0.02979815\n",
      " 0.02616622 0.02738061 0.02795278 0.02564812 0.02886573 0.0252133\n",
      " 0.0274222  0.02696473 0.02797869 0.02790925 0.0309737  0.02772783\n",
      " 0.02778293 0.02967554 0.02832444 0.02579386 0.02721585 0.03031076\n",
      " 0.02988492 0.03013917 0.02821051 0.02843128 0.02978393 0.02829433\n",
      " 0.03193771 0.02765344 0.02696802 0.02640644 0.02550479 0.02771454\n",
      " 0.02646687 0.0269343  0.02793819 0.02651518 0.03578219 0.02800255\n",
      " 0.02909659 0.02705553 0.02624594 0.02888972 0.0290153  0.0271897\n",
      " 0.02866531 0.02655344 0.03166614 0.02612966 0.02768508 0.02917567\n",
      " 0.02763807 0.02955641 0.0280362  0.02974699 0.03032999 0.02823573\n",
      " 0.02789259 0.02644737 0.02801266 0.02765664 0.0292262  0.0293443\n",
      " 0.02769955 0.03013425 0.02801029 0.02641526 0.02693969 0.02707233\n",
      " 0.02859811 0.02844523 0.02764896 0.02749277 0.0283411  0.02793906\n",
      " 0.02706755 0.02867368 0.02985558 0.02681105 0.02944303 0.0270063\n",
      " 0.02790391 0.02524976 0.0287633  0.02804096 0.02845699 0.02799514\n",
      " 0.02812594 0.02830161 0.02723996 0.02970446 0.02728647 0.02822146\n",
      " 0.02691814 0.0271767  0.02794006 0.02905476 0.0266244  0.02588118\n",
      " 0.02595709 0.02525472 0.02943049 0.02820281 0.02800188 0.0264544\n",
      " 0.0271124  0.02946062 0.02577256 0.02734901 0.02802316 0.029232\n",
      " 0.02729607 0.02931295 0.03046532 0.02695178 0.02794635 0.02896533\n",
      " 0.03072134 0.02717726 0.02820416 0.02670347 0.02868823 0.02860809\n",
      " 0.02665866 0.03071016 0.02984196 0.02979117 0.0297788  0.0288067\n",
      " 0.02759584 0.02835696 0.02740775 0.02870792 0.03081097 0.02801955\n",
      " 0.02832778 0.02646242 0.02869972 0.02770577 0.02925401 0.02801208\n",
      " 0.02689286 0.03139996 0.03065793 0.02753459 0.03030874 0.02731539\n",
      " 0.02722444 0.0261979  0.03108126 0.02798344 0.02918556 0.02701395\n",
      " 0.02859079 0.02910658 0.03603784 0.02591524 0.02826574 0.02934618\n",
      " 0.02985529 0.02552434 0.02936818 0.02901939 0.02775927 0.02827581\n",
      " 0.02792221 0.02691069 0.02834219 0.02966945 0.02772571 0.02626047\n",
      " 0.02642365 0.02810308 0.0262747  0.02819752 0.0265296  0.02771405\n",
      " 0.02867567 0.02943774 0.02858899 0.02670192 0.02646112 0.02927199\n",
      " 0.02929979 0.02842656 0.02832009 0.0299755  0.02870559 0.02806446\n",
      " 0.02622867 0.02760996 0.02969784 0.02894049 0.02787529 0.0267827\n",
      " 0.02796961 0.0264955  0.02804933 0.02797353 0.0274745  0.02834059\n",
      " 0.03008422 0.02766524 0.02908817 0.02786211 0.03052487 0.02709852\n",
      " 0.02849206 0.02645516 0.02683244 0.02672826 0.02671339 0.02828749\n",
      " 0.02679196 0.02777131 0.02579324 0.02776467 0.02671189 0.02826493\n",
      " 0.02712291 0.02912396 0.03400739 0.02697118 0.02658949 0.03460413\n",
      " 0.02943205 0.02848141 0.02850624 0.02946545 0.02733142 0.02895772\n",
      " 0.02727601 0.02727876 0.02414064 0.02651509 0.0307794  0.0285626\n",
      " 0.02859713 0.02969329 0.02985743 0.02700628 0.03034853 0.02522147\n",
      " 0.02820444 0.02961291 0.02673512 0.02585165 0.02726846 0.0251582\n",
      " 0.02783449 0.02722685 0.02876758 0.02721689 0.02547215 0.02887397\n",
      " 0.02690067 0.02798756 0.02710124 0.02835927 0.02726573 0.02810737\n",
      " 0.02752006 0.02915157 0.02754351 0.0265666  0.02838193 0.02941633\n",
      " 0.02912748 0.02639296 0.02782117 0.02780781 0.02638354 0.02879201\n",
      " 0.02641516 0.03237052 0.02566258 0.02931745 0.02650024 0.02756207\n",
      " 0.02666507 0.02704749 0.02718896 0.02895865 0.02856551 0.02753209\n",
      " 0.02844681 0.02691074 0.02692322 0.02990106 0.02686207 0.02796958\n",
      " 0.02929828 0.02912025 0.02760646 0.02831068 0.02704407 0.02793365\n",
      " 0.02782698 0.02885618 0.03015823 0.02775384 0.02662563 0.02828021\n",
      " 0.03021787 0.02818444 0.02826979 0.02517334 0.02652948 0.02355663\n",
      " 0.02693727 0.02648683 0.02579271 0.0283192  0.02514208 0.02674358\n",
      " 0.02727373 0.02792527 0.02895162 0.02795956 0.02931517 0.02746142\n",
      " 0.02734286 0.02720182]\n",
      "Pruned indices for layer 'block5_conv1': [1, 9, 13, 20, 31, 43, 47, 50, 63, 74, 90, 116, 124, 138, 139, 141, 177, 195, 197, 207, 220, 271, 287, 288, 289, 296, 345, 349, 404, 422, 431, 435, 437, 442, 464, 495, 497, 500, 502]\n",
      "[0.02768636 0.02444103 0.02750078 0.0266363  0.02985918 0.02845168\n",
      " 0.02754006 0.02722132 0.02829266 0.02682934 0.02609272 0.0296717\n",
      " 0.02900215 0.02937088 0.0282588  0.02755002 0.02846549 0.02601459\n",
      " 0.02777383 0.02720727 0.02834434 0.02748048 0.0282602  0.02752366\n",
      " 0.02674268 0.02873546 0.02826512 0.03007856 0.02856906 0.02825294\n",
      " 0.02901505 0.02771092 0.02827207 0.02866206 0.02796335 0.02898914\n",
      " 0.02731925 0.02687922 0.02729469 0.02692791 0.02697436 0.02761608\n",
      " 0.02725268 0.02731108 0.03029243 0.02743853 0.02623007 0.02857734\n",
      " 0.02623412 0.02820581 0.02811647 0.02589157 0.02471878 0.0309425\n",
      " 0.03062127 0.03630137 0.02866626 0.02601138 0.02972206 0.02641284\n",
      " 0.03399517 0.02730037 0.03034855 0.02771673 0.02775875 0.02761295\n",
      " 0.02857292 0.028483   0.02997124 0.0280687  0.0289941  0.02761269\n",
      " 0.02636587 0.02766502 0.02762662 0.02740461 0.02872518 0.029046\n",
      " 0.03056626 0.0289468  0.02662605 0.02902733 0.02716566 0.02809298\n",
      " 0.02581086 0.0281119  0.03019841 0.02685697 0.02639665 0.03081713\n",
      " 0.0283981  0.02708463 0.02812834 0.03121788 0.0269129  0.02686465\n",
      " 0.02814827 0.02568896 0.03735341 0.02922788 0.02533106 0.02882633\n",
      " 0.02761254 0.02889093 0.02673976 0.02808499 0.02651742 0.02956506\n",
      " 0.03450733 0.03157757 0.02711938 0.02676343 0.02777759 0.02872356\n",
      " 0.02545812 0.02645909 0.02845411 0.02951043 0.02852487 0.028139\n",
      " 0.02611173 0.02689003 0.0280681  0.02521426 0.02773388 0.02703676\n",
      " 0.02959743 0.02843325 0.02650998 0.02769951 0.02883073 0.0294936\n",
      " 0.02841017 0.02542767 0.02923298 0.02766507 0.03012755 0.02929159\n",
      " 0.02973071 0.02953516 0.02639387 0.02634744 0.02755507 0.02956713\n",
      " 0.02748855 0.02688987 0.02994162 0.0246187  0.02775306 0.03196076\n",
      " 0.02908312 0.02833521 0.02850526 0.02654619 0.02581443 0.02667368\n",
      " 0.02712421 0.02559622 0.0272988  0.02559125 0.02587194 0.02641547\n",
      " 0.02816305 0.0285003  0.02417768 0.02845388 0.02932614 0.02906547\n",
      " 0.02910497 0.03172524 0.02895269 0.02976436 0.02818313 0.02808664\n",
      " 0.02741702 0.03200502 0.02749383 0.02729472 0.03296086 0.02817059\n",
      " 0.02686018 0.02897175 0.02694442 0.02743708 0.02715771 0.02822253\n",
      " 0.0272529  0.03131674 0.0264112  0.03162144 0.02689229 0.02706313\n",
      " 0.02992969 0.02541257 0.02926659 0.02728026 0.02522572 0.02789737\n",
      " 0.02787268 0.03027787 0.03070131 0.02728085 0.02773538 0.02528398\n",
      " 0.02447988 0.02759842 0.02665499 0.02597667 0.02831445 0.02639983\n",
      " 0.02831491 0.02645048 0.02795506 0.02761992 0.02677246 0.02634482\n",
      " 0.02789787 0.02735954 0.02830321 0.02996839 0.02966891 0.0278961\n",
      " 0.02874635 0.02728162 0.03236204 0.02930123 0.02994834 0.02898072\n",
      " 0.02911742 0.02601864 0.02654906 0.02911665 0.02567399 0.02623514\n",
      " 0.0280651  0.0291016  0.02917589 0.02775089 0.02855318 0.02755537\n",
      " 0.0277643  0.02753414 0.02851923 0.02779981 0.02840856 0.0258818\n",
      " 0.0277467  0.02717781 0.03021022 0.03085769 0.02587777 0.02736871\n",
      " 0.02692036 0.03175683 0.02889405 0.02951524 0.02501339 0.02892877\n",
      " 0.02863538 0.02840386 0.02674682 0.02826235 0.03009673 0.02774687\n",
      " 0.02751671 0.03078918 0.02774038 0.02998558 0.0302726  0.02932392\n",
      " 0.02701983 0.031792   0.02652699 0.02916858 0.02980179 0.02564184\n",
      " 0.02867566 0.02654504 0.0289868  0.02686881 0.0284001  0.02769542\n",
      " 0.02727029 0.02992734 0.02591205 0.02738128 0.02525415 0.02691072\n",
      " 0.02973395 0.02580122 0.02869043 0.02726255 0.02641826 0.0277137\n",
      " 0.02760624 0.02945233 0.02701111 0.02638035 0.03051807 0.02597388\n",
      " 0.02890275 0.02791095 0.02712308 0.03103383 0.02590197 0.028657\n",
      " 0.02676314 0.02615595 0.02845784 0.03108878 0.02550052 0.02732749\n",
      " 0.02935587 0.02653458 0.02672625 0.02747295 0.02858839 0.02791775\n",
      " 0.02823849 0.0296459  0.02925583 0.02619315 0.02547332 0.02926087\n",
      " 0.02773641 0.02858288 0.02878283 0.03039424 0.02710807 0.03095892\n",
      " 0.02856739 0.02822384 0.02837299 0.02385035 0.02848607 0.02811555\n",
      " 0.02889943 0.02807891 0.03017602 0.02789852 0.02826377 0.02978474\n",
      " 0.02892362 0.02834339 0.02912965 0.02696091 0.02813691 0.02808057\n",
      " 0.02645623 0.02568229 0.02737634 0.02666203 0.02730357 0.02717782\n",
      " 0.02688555 0.03154032 0.02824382 0.02728766 0.02640598 0.02866247\n",
      " 0.02984137 0.02531838 0.02666632 0.02810996 0.02788905 0.02942366\n",
      " 0.02979513 0.0248265  0.02806204 0.03023823 0.02917119 0.02926643\n",
      " 0.02897289 0.02693896 0.02893474 0.02724308 0.03324929 0.02853131\n",
      " 0.02623768 0.02783802 0.02932429 0.02756381 0.02535947 0.02766642\n",
      " 0.02965212 0.02667995 0.02648922 0.02807153 0.02778068 0.02708945\n",
      " 0.02649946 0.02911191 0.02875679 0.02800137 0.02638944 0.02687007\n",
      " 0.02837011 0.02844388 0.02953916 0.03010476 0.02737016 0.02819595\n",
      " 0.02891985 0.02840908 0.02909853 0.02947875 0.02753323 0.02724205\n",
      " 0.02803578 0.02781946 0.02850487 0.02912497 0.02795616 0.02686194\n",
      " 0.02777303 0.0261099  0.0252599  0.02771558 0.02698667 0.02833821\n",
      " 0.02889583 0.02464137 0.02774238 0.02862714 0.02826363 0.02657324\n",
      " 0.02808756 0.03332867 0.02608448 0.02626281 0.02724968 0.03055316\n",
      " 0.02830056 0.03072632 0.02817325 0.02528255 0.02923013 0.03069169\n",
      " 0.03047164 0.0266046  0.02755544 0.02914546 0.02698248 0.02675964\n",
      " 0.02928687 0.02480108 0.02825522 0.02890143 0.02801462 0.02746427\n",
      " 0.03046183 0.02881623 0.02805585 0.02827596 0.02661448 0.02785079\n",
      " 0.02503761 0.02950949 0.02695427 0.03075733 0.03229207 0.0267467\n",
      " 0.02423163 0.0294448  0.02885987 0.02633986 0.02890358 0.02589634\n",
      " 0.02806689 0.03251499 0.02927231 0.02635445 0.02853044 0.02841879\n",
      " 0.02654881 0.02977706 0.0298564  0.02492048 0.02697906 0.02866452\n",
      " 0.02914969 0.0304279  0.026884   0.02862522 0.02704482 0.02886112\n",
      " 0.02701163 0.02976535 0.02679714 0.02652685 0.02913196 0.02514569\n",
      " 0.02841092 0.02810812 0.022067   0.02911673 0.02754029 0.02826699\n",
      " 0.02663385 0.02740503 0.02826922 0.02627224 0.02947037 0.02832014\n",
      " 0.0273499  0.02847597 0.02877988 0.03099269 0.02748334 0.02828331\n",
      " 0.02582934 0.02877222]\n",
      "Pruned indices for layer 'block5_conv2': [1, 51, 52, 84, 97, 100, 114, 123, 133, 147, 154, 157, 159, 160, 164, 193, 196, 203, 204, 207, 232, 245, 250, 256, 275, 284, 286, 289, 299, 304, 310, 322, 333, 349, 361, 367, 382, 416, 421, 435, 445, 456, 462, 467, 477, 491, 494, 510]\n",
      "[0.03099282 0.03227903 0.02772739 0.02819625 0.02570037 0.02969533\n",
      " 0.02969944 0.02772727 0.03145139 0.03081734 0.03026157 0.02965182\n",
      " 0.02834651 0.03140557 0.0274459  0.02889181 0.02881057 0.02409804\n",
      " 0.02818724 0.02748573 0.02582644 0.02646686 0.02787775 0.02923108\n",
      " 0.02734515 0.02777356 0.03064698 0.02946752 0.02514769 0.03214297\n",
      " 0.02489809 0.03016959 0.02386204 0.02693758 0.02472955 0.02813581\n",
      " 0.02917723 0.03030682 0.03124827 0.02793026 0.02561289 0.02843145\n",
      " 0.01995918 0.02896618 0.03161472 0.02527341 0.02618293 0.02883111\n",
      " 0.02955537 0.02819133 0.02580806 0.02483296 0.0252542  0.0285392\n",
      " 0.03161391 0.0209771  0.0271609  0.02870267 0.02624011 0.03050413\n",
      " 0.03313133 0.02909928 0.02696671 0.02719015 0.03026965 0.02931629\n",
      " 0.03317262 0.02571861 0.03060981 0.02959791 0.03431072 0.02787973\n",
      " 0.02576164 0.02840327 0.02923202 0.02735472 0.02600935 0.02837408\n",
      " 0.02779027 0.02842584 0.02806033 0.02821036 0.02713665 0.02826504\n",
      " 0.02894979 0.02737248 0.02457996 0.02626687 0.02531303 0.0243258\n",
      " 0.03182695 0.02899962 0.0320813  0.02725942 0.02560796 0.02821491\n",
      " 0.02536104 0.03231892 0.02755017 0.02677984 0.02820139 0.02616019\n",
      " 0.02891287 0.02667043 0.02735488 0.02901425 0.02805539 0.02534379\n",
      " 0.02925543 0.02846216 0.02654998 0.03052262 0.02901048 0.02565723\n",
      " 0.02798264 0.03421136 0.02662346 0.02806839 0.02377564 0.02577841\n",
      " 0.02671296 0.02948302 0.03068422 0.02297551 0.02516003 0.02828637\n",
      " 0.03334492 0.02486791 0.02718628 0.02922877 0.02594787 0.02890986\n",
      " 0.02878751 0.02554346 0.03337246 0.03106358 0.02903228 0.03225918\n",
      " 0.0283864  0.03096118 0.03176691 0.03262133 0.0283642  0.02964528\n",
      " 0.03413681 0.03244028 0.0244283  0.02843584 0.02895798 0.02837118\n",
      " 0.02757846 0.03024537 0.0257895  0.02753304 0.02521263 0.02175404\n",
      " 0.02693191 0.02930769 0.03130804 0.02814825 0.02942625 0.02928039\n",
      " 0.02969769 0.02762849 0.02726483 0.02780979 0.02799294 0.0317249\n",
      " 0.0263147  0.0297699  0.02980122 0.02933752 0.02548021 0.02715061\n",
      " 0.02797073 0.03006615 0.02782408 0.02984361 0.02919321 0.02522841\n",
      " 0.02597773 0.02899912 0.02877871 0.02525714 0.0282494  0.02978129\n",
      " 0.02286078 0.02439426 0.02819333 0.02779331 0.03131279 0.02543788\n",
      " 0.02701514 0.02687461 0.02675815 0.02938392 0.02440644 0.03091021\n",
      " 0.03059437 0.02726846 0.03346051 0.02762694 0.02912697 0.03148258\n",
      " 0.02855332 0.02752836 0.02949075 0.02596322 0.03028746 0.02690665\n",
      " 0.02576266 0.0288906  0.02606048 0.0327279  0.03001177 0.02292923\n",
      " 0.02876986 0.02605254 0.02649308 0.03040589 0.02534471 0.02517567\n",
      " 0.02699899 0.02975696 0.02744166 0.0274974  0.02555075 0.02635992\n",
      " 0.02910074 0.02407033 0.02459427 0.02909068 0.02724854 0.02857537\n",
      " 0.02593521 0.02820099 0.02715582 0.02647918 0.02941922 0.02907461\n",
      " 0.02712944 0.02604567 0.02896208 0.03172674 0.0260324  0.02723901\n",
      " 0.02731814 0.03259316 0.02971685 0.02663895 0.02372099 0.02997299\n",
      " 0.02633924 0.02848245 0.02572643 0.0260949  0.02814622 0.02656814\n",
      " 0.02754236 0.0264938  0.02679471 0.0267837  0.02700264 0.02831437\n",
      " 0.02444221 0.02783872 0.02524062 0.02935624 0.03186778 0.02507839\n",
      " 0.03049256 0.02949608 0.02790811 0.03100426 0.03072629 0.02787724\n",
      " 0.02217142 0.02520844 0.0263865  0.02682069 0.02713377 0.02829848\n",
      " 0.02489249 0.02746657 0.02797822 0.02804541 0.03414331 0.02996809\n",
      " 0.02788893 0.0275557  0.02554066 0.02863598 0.02807864 0.02816414\n",
      " 0.02741308 0.0305965  0.02396029 0.02894929 0.02742187 0.03179349\n",
      " 0.0271979  0.03042258 0.02532592 0.0271896  0.0307107  0.0315313\n",
      " 0.02707107 0.02541099 0.02982563 0.02815683 0.03181682 0.03035083\n",
      " 0.02786119 0.03056919 0.03202358 0.02472687 0.02602839 0.03043542\n",
      " 0.03192947 0.02374779 0.02662012 0.0234881  0.03083904 0.02917474\n",
      " 0.02671725 0.03292214 0.02561937 0.02844387 0.02461207 0.03408351\n",
      " 0.02881397 0.02701714 0.02807465 0.02691091 0.02565208 0.02761397\n",
      " 0.02845481 0.02705734 0.02754643 0.02910903 0.02586551 0.02771555\n",
      " 0.02669354 0.01990111 0.02746782 0.02320145 0.02574879 0.02557935\n",
      " 0.03234331 0.03226395 0.02648199 0.03433898 0.02409313 0.03141623\n",
      " 0.02754744 0.03109073 0.02837408 0.03099775 0.03139662 0.02829274\n",
      " 0.02979725 0.02926928 0.02842525 0.02714908 0.03303017 0.02825119\n",
      " 0.03257292 0.02747834 0.03083421 0.02704882 0.03252517 0.02554446\n",
      " 0.02497273 0.02832623 0.02773521 0.02922627 0.03173845 0.03192309\n",
      " 0.02769312 0.02647697 0.02858787 0.02579825 0.02253283 0.03085808\n",
      " 0.02940445 0.02851794 0.02650416 0.0261237  0.02579908 0.02699125\n",
      " 0.02820333 0.0268179  0.02931295 0.02831425 0.02929636 0.0271551\n",
      " 0.03063846 0.0304768  0.02799028 0.03026785 0.03057703 0.02829294\n",
      " 0.02472712 0.03198146 0.03236116 0.02705272 0.02798824 0.0276067\n",
      " 0.03014452 0.03035662 0.03024027 0.03211962 0.02443661 0.02805333\n",
      " 0.02718162 0.02845884 0.02464318 0.02564305 0.0275737  0.02879123\n",
      " 0.0247318  0.02860356 0.02820122 0.02954344 0.02707719 0.02210883\n",
      " 0.03110308 0.02946426 0.02652268 0.02863045 0.02545519 0.03098896\n",
      " 0.02443844 0.02935814 0.02457847 0.03184798 0.02807103 0.03292954\n",
      " 0.0253314  0.0290049  0.0281692  0.02342486 0.03088879 0.02731327\n",
      " 0.02825513 0.02635339 0.0313065  0.02840237 0.02778794 0.02753091\n",
      " 0.02572755 0.02608372 0.02937471 0.02705373 0.03008004 0.02729826\n",
      " 0.02678403 0.02892904 0.02589075 0.0315416  0.0293049  0.02661353\n",
      " 0.03076918 0.02873109 0.02644648 0.02464516 0.03045767 0.0313957\n",
      " 0.02619766 0.0223284  0.02555802 0.02803303 0.02708079 0.03133119\n",
      " 0.02857838 0.02955863 0.03473843 0.02375366 0.03219162 0.0262253\n",
      " 0.02667647 0.03240159 0.02440318 0.02423494 0.03353427 0.02891619\n",
      " 0.02405196 0.0280194  0.0271289  0.02764335 0.03192081 0.02096672\n",
      " 0.02990495 0.02726263 0.03135655 0.02291843 0.02673514 0.02754235\n",
      " 0.02361746 0.03012233 0.0258561  0.03196527 0.02544027 0.02446296\n",
      " 0.02209632 0.02655176 0.02983913 0.02345341 0.02951895 0.02701566\n",
      " 0.02142638 0.02672314]\n",
      "Pruned indices for layer 'block5_conv3': [4, 17, 20, 28, 30, 32, 34, 40, 42, 45, 50, 51, 52, 55, 67, 72, 86, 88, 89, 94, 96, 107, 113, 118, 119, 123, 124, 127, 130, 133, 146, 152, 154, 155, 172, 179, 180, 183, 186, 187, 191, 196, 207, 210, 215, 220, 221, 226, 229, 230, 234, 250, 254, 264, 266, 269, 276, 277, 282, 290, 296, 302, 307, 315, 319, 321, 326, 328, 334, 340, 343, 345, 346, 347, 352, 371, 372, 381, 382, 388, 402, 412, 416, 417, 420, 425, 430, 432, 434, 438, 441, 450, 458, 465, 469, 470, 477, 482, 483, 486, 491, 495, 498, 500, 502, 503, 504, 507, 510]\n",
      "Deleting 0/64 channels from layer: block1_conv1\n",
      "Deleting 0/64 channels from layer: block1_conv2\n",
      "Deleting 3/128 channels from layer: block2_conv1\n",
      "Deleting 0/128 channels from layer: block2_conv2\n",
      "Deleting 0/256 channels from layer: block3_conv1\n",
      "Deleting 0/256 channels from layer: block3_conv2\n",
      "Deleting 0/256 channels from layer: block3_conv3\n",
      "Deleting 237/512 channels from layer: block4_conv1\n",
      "Deleting 98/512 channels from layer: block4_conv2\n",
      "Deleting 99/512 channels from layer: block4_conv3\n",
      "Deleting 39/512 channels from layer: block5_conv1\n",
      "Deleting 48/512 channels from layer: block5_conv2\n",
      "Deleting 109/512 channels from layer: block5_conv3\n",
      "Model: \"model_55\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_26 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " input_40 (InputLayer)       multiple                  0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 125)     72125     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     144128    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 275)       633875    \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 414)       1025064   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 413)       1539251   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  multiple                  0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 473)       1758614   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 464)       1975712   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 403)       1683331   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  multiple                  0         \n",
      "                                                                 \n",
      " global_max_pool (GlobalMax  multiple                  0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 2)                 808       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10346956 (39.47 MB)\n",
      "Trainable params: 808 (3.16 KB)\n",
      "Non-trainable params: 10346148 (39.47 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:03:11.769458Z",
     "start_time": "2024-11-27T22:03:11.753832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the model\n",
    "pruningmodel.compile(loss=losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer = optimizers.Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n"
   ],
   "id": "cf7369ddf137dde1",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:03:54.328013Z",
     "start_time": "2024-11-27T22:03:13.756336Z"
    }
   },
   "cell_type": "code",
   "source": "pruningmodel.evaluate(validation_batches)",
   "id": "b4a000b6116afd1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 41s 4s/step - loss: 2.6552 - accuracy: 0.8669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.6551754474639893, 0.8668830990791321]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:16:33.624225Z",
     "start_time": "2024-11-27T22:16:33.615521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tempfile\n",
    "def prune_model(model, epochs,threshold, train_data=train_batches, val_data=validation_batches,  feature_extractor=model):\n",
    "\n",
    "  # Create a tensorboard logfile\n",
    "  logdir = tempfile.mkdtemp()\n",
    "  # The end_step is the total number of iterations required for the training data which is basically the entire epochs over the length of the training data\n",
    "  end_step = int(len(train_data) * epochs * 0.5)\n",
    "\n",
    "  learning_rate_fn = optimizers.schedules.PolynomialDecay(\n",
    "    0.001,\n",
    "    1000,\n",
    "    0.0001,\n",
    "    power=0.5)\n",
    "\n",
    "  # Model for pruning\n",
    "  entropy_pruning_instance = EntropyPruningSurgeon(model=model, threshold=threshold)\n",
    "  model_for_pruning = entropy_pruning_instance.run()\n",
    "\n",
    "  # Recompile\n",
    "  model_for_pruning.compile(optimizer= optimizers.Adam(),\n",
    "                            loss= losses.SparseCategoricalCrossentropy(),\n",
    "                            metrics=[\"accuracy\"])\n",
    "\n",
    "  # Fit the model\n",
    "  model_for_pruning.fit(train_data,\n",
    "                      validation_data=val_data,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=epochs)\n",
    "\n",
    "  # Save the model\n",
    "  #model_for_pruning.save(f\"mnist_model_sparsity_{final_sparsity}\")\n",
    "\n",
    "  # Evaluate the model\n",
    "  score = model_for_pruning.evaluate(val_data, verbose=0)\n",
    "  metric_dict = {\n",
    "      \"sparsity\" : final_sparsity,\n",
    "      \"val_loss\" : np.round(score[0], 4),\n",
    "      \"val_accuracy\" : np.round(score[1] * 100, 4)\n",
    "  }\n",
    "  return logdir, metric_dict, model_for_pruning"
   ],
   "id": "d3424e5788e68c42",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:17:25.747316Z",
     "start_time": "2024-11-27T22:16:54.056667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Choose the best entropy level\n",
    "k_entropies = np.arange(0.01, 0.031, 0.001)\n",
    "\n",
    "#k_sparsities = [0.50]\n",
    "metric_list = []\n",
    "\n",
    "for k in k_entropies:\n",
    "  # Load in the best saved model\n",
    "  model_1 = models.load_model(\"model_experiments/model_horses_or_humans.keras\")\n",
    "  logdir, metrics, pruned_model = prune_model(model=model_1,\n",
    "            threshold=k,\n",
    "            epochs=20)\n",
    "  val_loss, val_accuracy  = metrics[\"val_loss\"], metrics[\"val_accuracy\"]\n",
    "  metric_list.append(metrics)\n",
    "  print(f\"Sparsity : {k} \\tValidation Loss: {val_loss}, \\tValidation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Create a dataframe of the values obtained\n",
    "df = pd.DataFrame(metric_list)"
   ],
   "id": "b58a8495f4cc4ce4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 3/23 [==>...........................] - ETA: 1:46 - loss: 0.0000e+00 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m k_entropies:\n\u001B[0;32m      8\u001B[0m   \u001B[38;5;66;03m# Load in the best saved model\u001B[39;00m\n\u001B[0;32m      9\u001B[0m   model_1 \u001B[38;5;241m=\u001B[39m models\u001B[38;5;241m.\u001B[39mload_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_experiments/model_horses_or_humans.keras\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m   logdir, metrics, pruned_model \u001B[38;5;241m=\u001B[39m \u001B[43mprune_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m            \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m            \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m   val_loss, val_accuracy  \u001B[38;5;241m=\u001B[39m metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m], metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     14\u001B[0m   metric_list\u001B[38;5;241m.\u001B[39mappend(metrics)\n",
      "Cell \u001B[1;32mIn[53], line 25\u001B[0m, in \u001B[0;36mprune_model\u001B[1;34m(model, epochs, threshold, train_data, val_data, feature_extractor)\u001B[0m\n\u001B[0;32m     20\u001B[0m model_for_pruning\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m optimizers\u001B[38;5;241m.\u001B[39mAdam(),\n\u001B[0;32m     21\u001B[0m                           loss\u001B[38;5;241m=\u001B[39m losses\u001B[38;5;241m.\u001B[39mSparseCategoricalCrossentropy(),\n\u001B[0;32m     22\u001B[0m                           metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Fit the model\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m \u001B[43mmodel_for_pruning\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Save the model\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m#model_for_pruning.save(f\"mnist_model_sparsity_{final_sparsity}\")\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m     34\u001B[0m score \u001B[38;5;241m=\u001B[39m model_for_pruning\u001B[38;5;241m.\u001B[39mevaluate(val_data, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:1804\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1796\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1797\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1798\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1801\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1802\u001B[0m ):\n\u001B[0;32m   1803\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1804\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1805\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1806\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    830\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 833\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    835\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    836\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    866\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    867\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    868\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 869\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    870\u001B[0m \u001B[43m      \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_config\u001B[49m\n\u001B[0;32m    871\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    872\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    873\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    874\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    875\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[1;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[0;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[1;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[0;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[0;32m   1318\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1320\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1321\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1322\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1323\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1324\u001B[0m     args,\n\u001B[0;32m   1325\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1326\u001B[0m     executing_eagerly)\n\u001B[0;32m   1327\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[1;34m(self, args)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[1;32m~\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[0;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[1;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[0;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[0;32m    261\u001B[0m     )\n",
      "File \u001B[1;32m~\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001B[0m, in \u001B[0;36mContext.call_function\u001B[1;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[0;32m   1681\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[0;32m   1682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1683\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1684\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1685\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1686\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1687\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1688\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1689\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1690\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1691\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m   1692\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1693\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1697\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[0;32m   1698\u001B[0m   )\n",
      "File \u001B[1;32m~\\Documents\\Pro\\Projet\\tf1\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T18:22:59.096133Z",
     "start_time": "2024-11-26T18:22:59.079400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tf_keras import optimizers\n",
    "import tempfile\n",
    "import tensorflow_model_optimization as tfmot\n",
    "# Finish pruning after 2 epochs\n",
    "epochs = 2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def prune_model(model, initial_sparsity, final_sparsity, train_data=train_batches, val_data=validation_batches, epochs=epochs, feature_extractor=model ):\n",
    "\n",
    "  # Create a tensorboard logfile\n",
    "  logdir = tempfile.mkdtemp()\n",
    "  # The end_step is the total number of iterations required for the training data which is basically the entire epochs over the length of the training data\n",
    "  end_step = int(len(train_data) * epochs * 0.5)\n",
    "  # Import the low-magnitude-pruning function\n",
    "  prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "  # Set the prunung params\n",
    "  pruning_params = {\n",
    "\n",
    "      \"pruning_schedule\" : tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0,\n",
    "                                                                final_sparsity=final_sparsity,\n",
    "                                                                begin_step=0,\n",
    "                                                                end_step=end_step)\n",
    "\n",
    "                  }\n",
    "\n",
    "\n",
    "  learning_rate_fn = optimizers.schedules.PolynomialDecay(\n",
    "    0.001,\n",
    "    1000,\n",
    "    0.0001,\n",
    "    power=0.5)\n",
    "\n",
    "  # Model for pruning\n",
    "  #feature_extractor = prune_low_magnitude(feature_extractor, **pruning_params)\n",
    "  model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "  # Recompile\n",
    "  model_for_pruning.compile(optimizer= optimizers.Adam(),\n",
    "                            loss= losses.SparseCategoricalCrossentropy(),\n",
    "                            metrics=[\"accuracy\"])\n",
    "  #create callbacks\n",
    "  callbacks = [tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "              tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "               #create_model_checkpoint(model_name=model.name),\n",
    "              #tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                                  #patience=3,\n",
    "                                                  #verbose=1),\n",
    "               #tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                        #patience=4,\n",
    "                                                        #restore_best_weights=True)\n",
    "                                                        ]\n",
    "\n",
    "  # Fit the model\n",
    "  model_for_pruning.fit(train_data,\n",
    "                      validation_data=val_data,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=epochs,\n",
    "                      callbacks=callbacks)\n",
    "\n",
    "  # Save the model\n",
    "  #model_for_pruning.save(f\"mnist_model_sparsity_{final_sparsity}\")\n",
    "\n",
    "  # Evaluate the model\n",
    "  score = model_for_pruning.evaluate(val_data, verbose=0)\n",
    "  metric_dict = {\n",
    "      \"sparsity\" : final_sparsity,\n",
    "      \"val_loss\" : np.round(score[0], 4),\n",
    "      \"val_accuracy\" : np.round(score[1] * 100, 4)\n",
    "  }\n",
    "  return logdir, metric_dict, model_for_pruning"
   ],
   "id": "1b6ca5bc33f329d7",
   "outputs": [],
   "execution_count": 192
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T18:42:42.156292Z",
     "start_time": "2024-11-26T18:34:00.731545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Choose the best sparsity level\n",
    "final_sparsity = 0.80\n",
    "\n",
    "logdir, metrics, pruned_model = prune_model(model=pruned_model,\n",
    "          initial_sparsity=0,\n",
    "          final_sparsity=final_sparsity,\n",
    "          epochs=3)"
   ],
   "id": "cdc4c59b2c928e7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "23/23 [==============================] - 159s 7s/step - loss: 0.6922 - accuracy: 0.5257 - val_loss: 0.6947 - val_accuracy: 0.4838\n",
      "Epoch 2/3\n",
      "23/23 [==============================] - 174s 8s/step - loss: 0.6920 - accuracy: 0.5257 - val_loss: 0.6949 - val_accuracy: 0.4838\n",
      "Epoch 3/3\n",
      "23/23 [==============================] - 145s 6s/step - loss: 0.6920 - accuracy: 0.5257 - val_loss: 0.6949 - val_accuracy: 0.4838\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T18:31:41.121678Z",
     "start_time": "2024-11-26T18:31:40.682077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ],
   "id": "e6302f2afcdb5cfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neyen\\AppData\\Local\\Temp\\ipykernel_71452\\369930696.py:4: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned Keras model to: C:\\Users\\neyen\\AppData\\Local\\Temp\\tmpjh1lij65.h5\n"
     ]
    }
   ],
   "execution_count": 196
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1bb836c8b0e399f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
