{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-26T00:20:43.161890Z",
     "start_time": "2024-11-26T00:20:43.118307Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tf_keras import layers,losses,optimizers,Sequential\n",
    "from tf_keras.models import Model\n",
    "from tf_keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D,Input\n",
    "from tf_keras.applications import VGG16\n",
    "from tf_keras import datasets,models\n",
    "import importlib"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:20:45.971061Z",
     "start_time": "2024-11-26T00:20:45.964727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "id": "ac5bc649c1bea00",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:20:48.304569Z",
     "start_time": "2024-11-26T00:20:48.074533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from importing_dataset import load_dataset\n",
    "import Entropy\n",
    "importlib.reload(Entropy)\n",
    "from APoZ import APoZ_Algorithm\n",
    "train_examples, validation_examples, num_examples, num_classes, class_names = load_dataset('horses_or_humans', 70)"
   ],
   "id": "3dfca37d679c6ef0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:20:51.168722Z",
     "start_time": "2024-11-26T00:20:51.122685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from reformatting import reformat_image\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "train_batches = train_examples.cache().shuffle(num_examples//4).map(reformat_image).batch(BATCH_SIZE).prefetch(1)\n",
    "validation_batches = validation_examples.map(reformat_image).batch(BATCH_SIZE).prefetch(1)\n",
    "train_batches"
   ],
   "id": "cdc250b3c14c9cb1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:48:27.467484Z",
     "start_time": "2024-11-26T00:48:26.838697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialiser les couches une par une\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Bloc 1\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "# Bloc 2\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "# Bloc 3\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "# Bloc 4\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "# Bloc 5\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "# Ajouter des couches supplémentaires\n",
    "x = layers.GlobalMaxPooling2D(name=\"global_max_pool\")(x)\n",
    "x = layers.Dropout(0.3, name=\"dropout\")(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "# Construire le modèle\n",
    "model = Model(inputs, outputs, name=\"custom_vgg16.keras\")\n",
    "\n",
    "# Charger les poids de VGG16\n",
    "vgg16_base = VGG16(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "\n",
    "# Appliquer les poids pour chaque couche manuellement\n",
    "for layer, pretrained_layer in zip(model.layers, vgg16_base.layers):\n",
    "    if isinstance(layer, layers.Conv2D):\n",
    "        layer.set_weights(pretrained_layer.get_weights())\n"
   ],
   "id": "861e115df888413e",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T16:54:16.614449Z",
     "start_time": "2024-11-26T16:54:15.166340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Charger votre modèle\n",
    "importlib.reload(Entropy)\n",
    "from Entropy import EntropyPruning\n",
    "testt_model = models.load_model(\"model_experiments/custom_vgg16.keras\")\n",
    "testt_model.summary()"
   ],
   "id": "4ba92eb3e9fe6aba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_40 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_max_pool (GlobalMax  (None, 512)               0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14715714 (56.14 MB)\n",
      "Trainable params: 1026 (4.01 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T18:33:40.782962Z",
     "start_time": "2024-11-26T18:33:40.431053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pruning = EntropyPruning(model=testt_model, threshold=0.03)\n",
    "pruned_model = pruning.prune_model(None)\n",
    "pruned_model.summary()\n",
    "\n"
   ],
   "id": "b58a8495f4cc4ce4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  multiple                  0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  multiple                  0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  multiple                  0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  multiple                  0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  multiple                  0         \n",
      "                                                                 \n",
      " global_max_pool (GlobalMax  multiple                  0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14715714 (56.14 MB)\n",
      "Trainable params: 1026 (4.01 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T18:22:59.096133Z",
     "start_time": "2024-11-26T18:22:59.079400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tf_keras import optimizers\n",
    "import tempfile\n",
    "import tensorflow_model_optimization as tfmot\n",
    "# Finish pruning after 2 epochs\n",
    "epochs = 2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def prune_model(model, initial_sparsity, final_sparsity, train_data=train_batches, val_data=validation_batches, epochs=epochs, feature_extractor=model ):\n",
    "\n",
    "  # Create a tensorboard logfile\n",
    "  logdir = tempfile.mkdtemp()\n",
    "  # The end_step is the total number of iterations required for the training data which is basically the entire epochs over the length of the training data\n",
    "  end_step = int(len(train_data) * epochs * 0.5)\n",
    "  # Import the low-magnitude-pruning function\n",
    "  prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "  # Set the prunung params\n",
    "  pruning_params = {\n",
    "\n",
    "      \"pruning_schedule\" : tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0,\n",
    "                                                                final_sparsity=final_sparsity,\n",
    "                                                                begin_step=0,\n",
    "                                                                end_step=end_step)\n",
    "\n",
    "                  }\n",
    "\n",
    "\n",
    "  learning_rate_fn = optimizers.schedules.PolynomialDecay(\n",
    "    0.001,\n",
    "    1000,\n",
    "    0.0001,\n",
    "    power=0.5)\n",
    "\n",
    "  # Model for pruning\n",
    "  #feature_extractor = prune_low_magnitude(feature_extractor, **pruning_params)\n",
    "  model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "  # Recompile\n",
    "  model_for_pruning.compile(optimizer= optimizers.Adam(),\n",
    "                            loss= losses.SparseCategoricalCrossentropy(),\n",
    "                            metrics=[\"accuracy\"])\n",
    "  #create callbacks\n",
    "  callbacks = [tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "              tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "               #create_model_checkpoint(model_name=model.name),\n",
    "              #tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                                  #patience=3,\n",
    "                                                  #verbose=1),\n",
    "               #tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                        #patience=4,\n",
    "                                                        #restore_best_weights=True)\n",
    "                                                        ]\n",
    "\n",
    "  # Fit the model\n",
    "  model_for_pruning.fit(train_data,\n",
    "                      validation_data=val_data,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=epochs,\n",
    "                      callbacks=callbacks)\n",
    "\n",
    "  # Save the model\n",
    "  #model_for_pruning.save(f\"mnist_model_sparsity_{final_sparsity}\")\n",
    "\n",
    "  # Evaluate the model\n",
    "  score = model_for_pruning.evaluate(val_data, verbose=0)\n",
    "  metric_dict = {\n",
    "      \"sparsity\" : final_sparsity,\n",
    "      \"val_loss\" : np.round(score[0], 4),\n",
    "      \"val_accuracy\" : np.round(score[1] * 100, 4)\n",
    "  }\n",
    "  return logdir, metric_dict, model_for_pruning"
   ],
   "id": "1b6ca5bc33f329d7",
   "outputs": [],
   "execution_count": 192
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T18:42:42.156292Z",
     "start_time": "2024-11-26T18:34:00.731545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Choose the best sparsity level\n",
    "final_sparsity = 0.80\n",
    "\n",
    "logdir, metrics, pruned_model = prune_model(model=pruned_model,\n",
    "          initial_sparsity=0,\n",
    "          final_sparsity=final_sparsity,\n",
    "          epochs=3)"
   ],
   "id": "cdc4c59b2c928e7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "23/23 [==============================] - 159s 7s/step - loss: 0.6922 - accuracy: 0.5257 - val_loss: 0.6947 - val_accuracy: 0.4838\n",
      "Epoch 2/3\n",
      "23/23 [==============================] - 174s 8s/step - loss: 0.6920 - accuracy: 0.5257 - val_loss: 0.6949 - val_accuracy: 0.4838\n",
      "Epoch 3/3\n",
      "23/23 [==============================] - 145s 6s/step - loss: 0.6920 - accuracy: 0.5257 - val_loss: 0.6949 - val_accuracy: 0.4838\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T18:31:41.121678Z",
     "start_time": "2024-11-26T18:31:40.682077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ],
   "id": "e6302f2afcdb5cfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neyen\\AppData\\Local\\Temp\\ipykernel_71452\\369930696.py:4: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned Keras model to: C:\\Users\\neyen\\AppData\\Local\\Temp\\tmpjh1lij65.h5\n"
     ]
    }
   ],
   "execution_count": 196
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1bb836c8b0e399f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
